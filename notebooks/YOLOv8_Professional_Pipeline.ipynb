{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ YOLOv8 Professional Training Pipeline\n",
    "## CSV â†’ YOLO Conversion Â· Training Â· Evaluation Â· Export Â· Repository Builder\n",
    "\n",
    "**Features:**\n",
    "- Auto-detect CSV annotation format (pixel boxes, width/height, normalized YOLO)\n",
    "- Deterministic train/val split with configurable ratio\n",
    "- Full YOLOv8 training with Ultralytics\n",
    "- Evaluation, inference visualization, and model export (ONNX, TorchScript)\n",
    "\n",
    "---\n",
    "> ğŸ“Œ **How to use:** Run cells sequentially. Edit the **Configuration** cell to match your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Â· Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Install pinned dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "!pip install -q ultralytics==8.2.103 opencv-python-headless==4.10.0.84 \\\n",
    "    pandas==2.2.2 pyyaml==6.0.2 matplotlib==3.9.2 onnx==1.16.2 \\\n",
    "    onnxruntime==1.19.2 Pillow==10.4.0 tqdm==4.66.5 seaborn==0.13.2\n",
    "\n",
    "import ultralytics\n",
    "print(f\"Ultralytics version: {ultralytics.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Â· Configuration\n",
    "\n",
    "Edit the paths and hyperparameters below to match your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONFIGURATION â€” edit these values\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IMAGES_DIR   = \"/content/data/images\"       # folder with jpg/png images\n",
    "CSV_PATH     = \"/content/data/annotations.csv\"  # CSV annotation file\n",
    "WORK_DIR     = \"/content/work\"              # intermediate files\n",
    "OUTPUT_DIR   = \"/content/output\"            # final artifacts\n",
    "REPO_DIR     = \"/content/repo\"              # generated repository\n",
    "\n",
    "# â”€â”€ Train / Val split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SPLIT_RATIO  = 0.8     # fraction for training (rest â†’ validation)\n",
    "SEED         = 42\n",
    "\n",
    "# â”€â”€ Training hyperparameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MODEL_SIZE   = \"yolov8n.pt\"   # yolov8n / yolov8s / yolov8m / yolov8l / yolov8x\n",
    "IMGSZ        = 640\n",
    "EPOCHS       = 50\n",
    "BATCH        = 16\n",
    "DEVICE       = \"\"             # \"\" = auto, \"0\" = GPU 0, \"cpu\" = CPU\n",
    "\n",
    "# â”€â”€ Options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ALLOW_BACKGROUND_IMAGES = True   # keep images with no labels as background\n",
    "LICENSE_TYPE = \"MIT\"              # \"MIT\" or \"Apache-2.0\"\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"âœ… Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Â· Imports & Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, random, glob, pathlib, warnings, sys, yaml, csv, json\n",
    "from pathlib import Path\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# â”€â”€ Helper: get image dimensions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_image_size(path):\n",
    "    \"\"\"Return (width, height) of an image file.\"\"\"\n",
    "    with Image.open(path) as img:\n",
    "        return img.size  # (w, h)\n",
    "\n",
    "# â”€â”€ Helper: detect CSV format â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def detect_csv_format(df):\n",
    "    \"\"\"\n",
    "    Auto-detect CSV annotation format.\n",
    "    Returns one of: 'A' (xmin,ymin,xmax,ymax), 'B' (xmin,ymin,w,h),\n",
    "                     'C' (normalized YOLO), or raises ValueError.\n",
    "    \"\"\"\n",
    "    cols_lower = [c.lower().strip() for c in df.columns]\n",
    "\n",
    "    # Check for 'normalized' column â†’ Format C\n",
    "    if 'normalized' in cols_lower:\n",
    "        print(\"ğŸ“ Detected Format C (normalized YOLO-like)\")\n",
    "        return 'C'\n",
    "\n",
    "    # Check column names for xmax/ymax â†’ Format A\n",
    "    if 'xmax' in cols_lower and 'ymax' in cols_lower:\n",
    "        print(\"ğŸ“ Detected Format A (xmin, ymin, xmax, ymax)\")\n",
    "        return 'A'\n",
    "\n",
    "    # Check for width/height columns â†’ Format B\n",
    "    if 'width' in cols_lower and 'height' in cols_lower:\n",
    "        print(\"ğŸ“ Detected Format B (xmin, ymin, width, height)\")\n",
    "        return 'B'\n",
    "\n",
    "    # Fallback: try positional detection\n",
    "    if len(df.columns) >= 6:\n",
    "        # Check if values look normalized (all between 0 and 1)\n",
    "        numeric_cols = df.iloc[:, 1:5]\n",
    "        try:\n",
    "            numeric_vals = numeric_cols.astype(float)\n",
    "            if numeric_vals.max().max() <= 1.0 and numeric_vals.min().min() >= 0.0:\n",
    "                print(\"ğŸ“ Auto-detected Format C (values appear normalized)\")\n",
    "                return 'C'\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "        print(\"ğŸ“ Falling back to Format A (positional: col1-4 as xmin,ymin,xmax,ymax)\")\n",
    "        return 'A'\n",
    "\n",
    "    raise ValueError(\n",
    "        \"âŒ Cannot detect CSV format. Expected columns like:\\n\"\n",
    "        \"  Format A: filename, xmin, ymin, xmax, ymax, class\\n\"\n",
    "        \"  Format B: filename, xmin, ymin, width, height, class\\n\"\n",
    "        \"  Format C: filename, x_center, y_center, width, height, class, normalized\"\n",
    "    )\n",
    "\n",
    "# â”€â”€ Helper: normalize columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def normalize_columns(df, fmt):\n",
    "    \"\"\"Rename columns to a standard set based on detected format.\"\"\"\n",
    "    df = df.copy()\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    if fmt == 'A':\n",
    "        mapping = {}\n",
    "        cols_lower = {c.lower().strip(): c for c in cols}\n",
    "        for target, candidates in [\n",
    "            ('filename', ['filename', 'file', 'image', 'image_name', 'img']),\n",
    "            ('xmin', ['xmin', 'x_min', 'x1', 'left']),\n",
    "            ('ymin', ['ymin', 'y_min', 'y1', 'top']),\n",
    "            ('xmax', ['xmax', 'x_max', 'x2', 'right']),\n",
    "            ('ymax', ['ymax', 'y_max', 'y2', 'bottom']),\n",
    "            ('class', ['class', 'label', 'class_name', 'category', 'cls']),\n",
    "        ]:\n",
    "            for c in candidates:\n",
    "                if c in cols_lower:\n",
    "                    mapping[cols_lower[c]] = target\n",
    "                    break\n",
    "        if len(mapping) < 6:\n",
    "            # positional fallback\n",
    "            mapping = {cols[0]: 'filename', cols[1]: 'xmin', cols[2]: 'ymin',\n",
    "                       cols[3]: 'xmax', cols[4]: 'ymax', cols[5]: 'class'}\n",
    "        df = df.rename(columns=mapping)\n",
    "\n",
    "    elif fmt == 'B':\n",
    "        cols_lower = {c.lower().strip(): c for c in cols}\n",
    "        mapping = {}\n",
    "        for target, candidates in [\n",
    "            ('filename', ['filename', 'file', 'image', 'image_name', 'img']),\n",
    "            ('xmin', ['xmin', 'x_min', 'x1', 'left', 'x']),\n",
    "            ('ymin', ['ymin', 'y_min', 'y1', 'top', 'y']),\n",
    "            ('width', ['width', 'w', 'box_width']),\n",
    "            ('height', ['height', 'h', 'box_height']),\n",
    "            ('class', ['class', 'label', 'class_name', 'category', 'cls']),\n",
    "        ]:\n",
    "            for c in candidates:\n",
    "                if c in cols_lower:\n",
    "                    mapping[cols_lower[c]] = target\n",
    "                    break\n",
    "        if len(mapping) < 6:\n",
    "            mapping = {cols[0]: 'filename', cols[1]: 'xmin', cols[2]: 'ymin',\n",
    "                       cols[3]: 'width', cols[4]: 'height', cols[5]: 'class'}\n",
    "        df = df.rename(columns=mapping)\n",
    "\n",
    "    elif fmt == 'C':\n",
    "        cols_lower = {c.lower().strip(): c for c in cols}\n",
    "        mapping = {}\n",
    "        for target, candidates in [\n",
    "            ('filename', ['filename', 'file', 'image', 'image_name', 'img']),\n",
    "            ('x_center', ['x_center', 'cx', 'center_x', 'x']),\n",
    "            ('y_center', ['y_center', 'cy', 'center_y', 'y']),\n",
    "            ('width', ['width', 'w', 'box_width']),\n",
    "            ('height', ['height', 'h', 'box_height']),\n",
    "            ('class', ['class', 'label', 'class_name', 'category', 'cls']),\n",
    "        ]:\n",
    "            for c in candidates:\n",
    "                if c in cols_lower:\n",
    "                    mapping[cols_lower[c]] = target\n",
    "                    break\n",
    "        if len(mapping) < 6:\n",
    "            mapping = {cols[0]: 'filename', cols[1]: 'x_center', cols[2]: 'y_center',\n",
    "                       cols[3]: 'width', cols[4]: 'height', cols[5]: 'class'}\n",
    "        df = df.rename(columns=mapping)\n",
    "\n",
    "    # Normalize filename to basename\n",
    "    df['filename'] = df['filename'].apply(lambda x: os.path.basename(str(x).strip()))\n",
    "    return df\n",
    "\n",
    "# â”€â”€ Helper: convert to YOLO format â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def convert_to_yolo(row, fmt, img_w, img_h):\n",
    "    \"\"\"\n",
    "    Convert a single annotation row to YOLO format:\n",
    "    (x_center, y_center, width, height) all normalized to [0, 1].\n",
    "    Returns None if the box is invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if fmt == 'A':\n",
    "            xmin = float(row['xmin'])\n",
    "            ymin = float(row['ymin'])\n",
    "            xmax = float(row['xmax'])\n",
    "            ymax = float(row['ymax'])\n",
    "        elif fmt == 'B':\n",
    "            xmin = float(row['xmin'])\n",
    "            ymin = float(row['ymin'])\n",
    "            xmax = xmin + float(row['width'])\n",
    "            ymax = ymin + float(row['height'])\n",
    "        elif fmt == 'C':\n",
    "            # Already normalized\n",
    "            xc = float(row['x_center'])\n",
    "            yc = float(row['y_center'])\n",
    "            w  = float(row['width'])\n",
    "            h  = float(row['height'])\n",
    "            # Clamp\n",
    "            xc = max(0.0, min(1.0, xc))\n",
    "            yc = max(0.0, min(1.0, yc))\n",
    "            w  = max(0.0, min(1.0, w))\n",
    "            h  = max(0.0, min(1.0, h))\n",
    "            if w <= 0 or h <= 0:\n",
    "                return None\n",
    "            return (xc, yc, w, h)\n",
    "\n",
    "        # For A and B: clamp to image bounds\n",
    "        xmin = max(0, min(img_w, xmin))\n",
    "        ymin = max(0, min(img_h, ymin))\n",
    "        xmax = max(0, min(img_w, xmax))\n",
    "        ymax = max(0, min(img_h, ymax))\n",
    "\n",
    "        bw = xmax - xmin\n",
    "        bh = ymax - ymin\n",
    "        if bw <= 0 or bh <= 0:\n",
    "            return None\n",
    "\n",
    "        xc = (xmin + xmax) / 2.0 / img_w\n",
    "        yc = (ymin + ymax) / 2.0 / img_h\n",
    "        w  = bw / img_w\n",
    "        h  = bh / img_h\n",
    "        return (xc, yc, w, h)\n",
    "\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "# â”€â”€ Helper: build class mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_class_mapping(classes_series):\n",
    "    \"\"\"\n",
    "    Build a mapping from class label to integer ID.\n",
    "    If labels are already integers, validate contiguous from 0; remap if needed.\n",
    "    Returns: dict {label: int_id}, list of class names ordered by id.\n",
    "    \"\"\"\n",
    "    unique_classes = classes_series.unique()\n",
    "\n",
    "    # Check if all are integers\n",
    "    all_int = True\n",
    "    for c in unique_classes:\n",
    "        try:\n",
    "            int(c)\n",
    "        except (ValueError, TypeError):\n",
    "            all_int = False\n",
    "            break\n",
    "\n",
    "    if all_int:\n",
    "        int_classes = sorted([int(c) for c in unique_classes])\n",
    "        if int_classes == list(range(len(int_classes))):\n",
    "            mapping = {str(c): c for c in int_classes}\n",
    "            names = [str(c) for c in int_classes]\n",
    "        else:\n",
    "            # Remap\n",
    "            mapping = {str(c): i for i, c in enumerate(int_classes)}\n",
    "            names = [str(c) for c in int_classes]\n",
    "            print(f\"âš ï¸  Class IDs are not contiguous. Remapped: {mapping}\")\n",
    "    else:\n",
    "        sorted_classes = sorted([str(c) for c in unique_classes])\n",
    "        mapping = {c: i for i, c in enumerate(sorted_classes)}\n",
    "        names = sorted_classes\n",
    "\n",
    "    return mapping, names\n",
    "\n",
    "# â”€â”€ Helper: visualize boxes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def visualize_yolo_labels(image_path, label_path, class_names, ax=None):\n",
    "    \"\"\"Draw YOLO bounding boxes on an image.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    w, h = img.size\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(os.path.basename(image_path), fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, max(len(class_names), 1)))\n",
    "\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                cls_id = int(parts[0])\n",
    "                xc, yc, bw, bh = [float(x) for x in parts[1:]]\n",
    "                x1 = (xc - bw / 2) * w\n",
    "                y1 = (yc - bh / 2) * h\n",
    "                rect_w = bw * w\n",
    "                rect_h = bh * h\n",
    "                color = colors[cls_id % len(colors)]\n",
    "                rect = patches.Rectangle((x1, y1), rect_w, rect_h,\n",
    "                                         linewidth=2, edgecolor=color,\n",
    "                                         facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                label = class_names[cls_id] if cls_id < len(class_names) else str(cls_id)\n",
    "                ax.text(x1, y1 - 4, label, fontsize=8, color='white',\n",
    "                        bbox=dict(boxstyle='round,pad=0.2', facecolor=color, alpha=0.8))\n",
    "    return ax\n",
    "\n",
    "print(\"âœ… Utility functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Â· Data Validation & Inspection\n",
    "\n",
    "Read the CSV, auto-detect format, validate data, and display summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Read CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"ğŸ“„ CSV loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
    "print(f\"   Columns: {list(df.columns)}\")\n",
    "print()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Detect format & normalize columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "csv_format = detect_csv_format(df)\n",
    "df = normalize_columns(df, csv_format)\n",
    "print(f\"\\nğŸ“Š Format: {csv_format}\")\n",
    "print(f\"   Unique classes: {df['class'].nunique()}\")\n",
    "print(f\"   Class distribution:\")\n",
    "print(df['class'].value_counts().to_string())\n",
    "print(f\"\\n   Unique images in CSV: {df['filename'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Validate image files exist â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}\n",
    "available_images = {}\n",
    "for f in os.listdir(IMAGES_DIR):\n",
    "    if pathlib.Path(f).suffix.lower() in image_extensions:\n",
    "        available_images[f] = os.path.join(IMAGES_DIR, f)\n",
    "\n",
    "csv_filenames = set(df['filename'].unique())\n",
    "found = csv_filenames & set(available_images.keys())\n",
    "missing = csv_filenames - set(available_images.keys())\n",
    "\n",
    "print(f\"ğŸ–¼ï¸  Images in folder: {len(available_images)}\")\n",
    "print(f\"   Matched to CSV:   {len(found)}\")\n",
    "if missing:\n",
    "    print(f\"   âš ï¸  Missing files:  {len(missing)}\")\n",
    "    for m in list(missing)[:10]:\n",
    "        print(f\"      - {m}\")\n",
    "    if len(missing) > 10:\n",
    "        print(f\"      ... and {len(missing) - 10} more\")\n",
    "    # Remove rows with missing images\n",
    "    df = df[df['filename'].isin(found)]\n",
    "    print(f\"   â†’ Kept {len(df)} rows after removing missing images.\")\n",
    "\n",
    "assert len(found) > 0, \"âŒ No matching images found. Check IMAGES_DIR and CSV filenames.\"\n",
    "print(\"\\nâœ… Data validation passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Validate & clean bounding boxes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "initial_count = len(df)\n",
    "clamped_count = 0\n",
    "\n",
    "if csv_format in ('A', 'B'):\n",
    "    # Check for NaN in coordinate columns\n",
    "    coord_cols = ['xmin', 'ymin']\n",
    "    coord_cols += ['xmax', 'ymax'] if csv_format == 'A' else ['width', 'height']\n",
    "    df = df.dropna(subset=coord_cols + ['class', 'filename'])\n",
    "\n",
    "    # Convert to numeric\n",
    "    for col in coord_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df = df.dropna(subset=coord_cols)\n",
    "\n",
    "elif csv_format == 'C':\n",
    "    coord_cols = ['x_center', 'y_center', 'width', 'height']\n",
    "    df = df.dropna(subset=coord_cols + ['class', 'filename'])\n",
    "    for col in coord_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df = df.dropna(subset=coord_cols)\n",
    "\n",
    "dropped = initial_count - len(df)\n",
    "if dropped > 0:\n",
    "    print(f\"âš ï¸  Dropped {dropped} rows with NaN/invalid values.\")\n",
    "\n",
    "print(f\"âœ… {len(df)} valid annotation rows remaining.\")\n",
    "print(f\"   Covering {df['filename'].nunique()} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Â· YOLO Dataset Conversion Pipeline\n",
    "\n",
    "Create the YOLO directory structure, split into train/val, convert annotations, and generate `data.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Build class mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class_mapping, class_names = build_class_mapping(df['class'])\n",
    "num_classes = len(class_names)\n",
    "print(f\"ğŸ·ï¸  {num_classes} classes: {class_names}\")\n",
    "print(f\"   Mapping: {class_mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Create YOLO directory structure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DATASET_DIR = os.path.join(WORK_DIR, \"dataset\")\n",
    "for split in ['train', 'val']:\n",
    "    os.makedirs(os.path.join(DATASET_DIR, 'images', split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(DATASET_DIR, 'labels', split), exist_ok=True)\n",
    "\n",
    "# â”€â”€ Split images into train / val â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_images = sorted(df['filename'].unique())\n",
    "random.seed(SEED)\n",
    "random.shuffle(all_images)\n",
    "split_idx = int(len(all_images) * SPLIT_RATIO)\n",
    "train_images = set(all_images[:split_idx])\n",
    "val_images   = set(all_images[split_idx:])\n",
    "\n",
    "# Handle background images (images in folder but not in CSV)\n",
    "if ALLOW_BACKGROUND_IMAGES:\n",
    "    bg_images = set(available_images.keys()) - csv_filenames\n",
    "    if bg_images:\n",
    "        bg_list = sorted(bg_images)\n",
    "        random.shuffle(bg_list)\n",
    "        bg_split = int(len(bg_list) * SPLIT_RATIO)\n",
    "        train_images |= set(bg_list[:bg_split])\n",
    "        val_images   |= set(bg_list[bg_split:])\n",
    "        print(f\"ğŸ–¼ï¸  Added {len(bg_images)} background images (no labels).\")\n",
    "\n",
    "print(f\"ğŸ“‚ Train: {len(train_images)} images | Val: {len(val_images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Convert annotations & copy images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "stats = {'train': 0, 'val': 0, 'boxes': 0, 'skipped_boxes': 0}\n",
    "\n",
    "for img_name in tqdm(sorted(train_images | val_images), desc=\"Converting\"):\n",
    "    split = 'train' if img_name in train_images else 'val'\n",
    "    stats[split] += 1\n",
    "\n",
    "    # Copy image\n",
    "    src_path = available_images.get(img_name)\n",
    "    if src_path is None:\n",
    "        continue\n",
    "    dst_img = os.path.join(DATASET_DIR, 'images', split, img_name)\n",
    "    if not os.path.exists(dst_img):\n",
    "        shutil.copy2(src_path, dst_img)\n",
    "\n",
    "    # Get image dimensions\n",
    "    img_w, img_h = get_image_size(src_path)\n",
    "\n",
    "    # Get annotations for this image\n",
    "    img_df = df[df['filename'] == img_name]\n",
    "\n",
    "    # Write label file\n",
    "    label_name = os.path.splitext(img_name)[0] + '.txt'\n",
    "    label_path = os.path.join(DATASET_DIR, 'labels', split, label_name)\n",
    "\n",
    "    lines = []\n",
    "    for _, row in img_df.iterrows():\n",
    "        result = convert_to_yolo(row, csv_format, img_w, img_h)\n",
    "        if result is None:\n",
    "            stats['skipped_boxes'] += 1\n",
    "            continue\n",
    "        xc, yc, w, h = result\n",
    "        cls_id = class_mapping[str(row['class'])]\n",
    "        lines.append(f\"{cls_id} {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\")\n",
    "        stats['boxes'] += 1\n",
    "\n",
    "    with open(label_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(f\"\\nâœ… Conversion complete!\")\n",
    "print(f\"   Train images: {stats['train']}\")\n",
    "print(f\"   Val images:   {stats['val']}\")\n",
    "print(f\"   Total boxes:  {stats['boxes']}\")\n",
    "if stats['skipped_boxes'] > 0:\n",
    "    print(f\"   âš ï¸  Skipped boxes: {stats['skipped_boxes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Generate data.yaml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "data_yaml = {\n",
    "    'path': DATASET_DIR,\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'nc': num_classes,\n",
    "    'names': class_names,\n",
    "}\n",
    "\n",
    "data_yaml_path = os.path.join(DATASET_DIR, 'data.yaml')\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    yaml.dump(data_yaml, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"ğŸ“„ data.yaml written to: {data_yaml_path}\")\n",
    "print(\"â”€\" * 50)\n",
    "with open(data_yaml_path) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Â· Visualization\n",
    "\n",
    "Display sample training images with bounding boxes to verify the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Visualize sample training images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_img_dir = os.path.join(DATASET_DIR, 'images', 'train')\n",
    "train_lbl_dir = os.path.join(DATASET_DIR, 'labels', 'train')\n",
    "\n",
    "sample_images = [f for f in os.listdir(train_img_dir)\n",
    "                 if pathlib.Path(f).suffix.lower() in image_extensions]\n",
    "\n",
    "# Prefer images that have labels\n",
    "labeled = [f for f in sample_images\n",
    "           if os.path.getsize(os.path.join(train_lbl_dir,\n",
    "              os.path.splitext(f)[0] + '.txt')) > 0\n",
    "           if os.path.exists(os.path.join(train_lbl_dir,\n",
    "              os.path.splitext(f)[0] + '.txt'))]\n",
    "\n",
    "display_images = labeled[:5] if len(labeled) >= 5 else sample_images[:5]\n",
    "n = len(display_images)\n",
    "\n",
    "fig, axes = plt.subplots(1, n, figsize=(5 * n, 5))\n",
    "if n == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, img_name in zip(axes, display_images):\n",
    "    img_path = os.path.join(train_img_dir, img_name)\n",
    "    lbl_path = os.path.join(train_lbl_dir, os.path.splitext(img_name)[0] + '.txt')\n",
    "    visualize_yolo_labels(img_path, lbl_path, class_names, ax=ax)\n",
    "\n",
    "plt.suptitle(\"Sample Training Images with YOLO Bounding Boxes\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Â· YOLOv8 Training\n",
    "\n",
    "Train the model using Ultralytics. Metrics are logged automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# â”€â”€ Initialize model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model = YOLO(MODEL_SIZE)\n",
    "print(f\"ğŸ”§ Model: {MODEL_SIZE}\")\n",
    "print(f\"   Image size: {IMGSZ}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Batch: {BATCH}\")\n",
    "print(f\"   Device: {DEVICE if DEVICE else 'auto'}\")\n",
    "\n",
    "# â”€â”€ Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "results = model.train(\n",
    "    data=data_yaml_path,\n",
    "    imgsz=IMGSZ,\n",
    "    epochs=EPOCHS,\n",
    "    batch=BATCH,\n",
    "    device=DEVICE if DEVICE else None,\n",
    "    seed=SEED,\n",
    "    project=os.path.join(WORK_DIR, \"runs\"),\n",
    "    name=\"train\",\n",
    "    exist_ok=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Display training metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_dir = os.path.join(WORK_DIR, \"runs\", \"train\")\n",
    "\n",
    "# Show results plots if available\n",
    "for plot_name in ['results.png', 'confusion_matrix.png', 'confusion_matrix_normalized.png']:\n",
    "    plot_path = os.path.join(train_dir, plot_name)\n",
    "    if os.path.exists(plot_path):\n",
    "        img = Image.open(plot_path)\n",
    "        fig, ax = plt.subplots(figsize=(12, 8) if 'results' in plot_name else (8, 8))\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(plot_name, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Â· Evaluation & Inference\n",
    "\n",
    "Run validation and display predictions on sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_weights = os.path.join(train_dir, \"weights\", \"best.pt\")\n",
    "model_best = YOLO(best_weights)\n",
    "\n",
    "val_results = model_best.val(data=data_yaml_path, imgsz=IMGSZ, verbose=True)\n",
    "\n",
    "print(\"\\nğŸ“Š Validation Results:\")\n",
    "print(f\"   mAP50:    {val_results.box.map50:.4f}\")\n",
    "print(f\"   mAP50-95: {val_results.box.map:.4f}\")\n",
    "print(f\"   Precision: {val_results.box.mp:.4f}\")\n",
    "print(f\"   Recall:    {val_results.box.mr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Inference on sample validation images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "val_img_dir = os.path.join(DATASET_DIR, 'images', 'val')\n",
    "val_samples = [f for f in os.listdir(val_img_dir)\n",
    "               if pathlib.Path(f).suffix.lower() in image_extensions][:4]\n",
    "\n",
    "if val_samples:\n",
    "    fig, axes = plt.subplots(1, len(val_samples), figsize=(5 * len(val_samples), 5))\n",
    "    if len(val_samples) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, img_name in zip(axes, val_samples):\n",
    "        img_path = os.path.join(val_img_dir, img_name)\n",
    "        preds = model_best.predict(img_path, imgsz=IMGSZ, conf=0.25, verbose=False)\n",
    "        # Plot using ultralytics built-in\n",
    "        result_img = preds[0].plot()\n",
    "        ax.imshow(result_img[..., ::-1])  # BGR to RGB\n",
    "        ax.set_title(img_name, fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.suptitle(\"Predictions on Validation Images\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸  No validation images found for inference preview.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Â· Model Export\n",
    "\n",
    "Export the best model to ONNX and TorchScript formats, and save all artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Export to ONNX â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    onnx_path = model_best.export(format=\"onnx\", imgsz=IMGSZ)\n",
    "    print(f\"âœ… ONNX exported: {onnx_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  ONNX export failed: {e}\")\n",
    "    onnx_path = None\n",
    "\n",
    "# â”€â”€ Export to TorchScript â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    ts_path = model_best.export(format=\"torchscript\", imgsz=IMGSZ)\n",
    "    print(f\"âœ… TorchScript exported: {ts_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  TorchScript export failed: {e}\")\n",
    "    ts_path = None\n",
    "\n",
    "# â”€â”€ Copy artifacts to output directory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "shutil.copy2(best_weights, os.path.join(OUTPUT_DIR, \"best.pt\"))\n",
    "shutil.copy2(data_yaml_path, os.path.join(OUTPUT_DIR, \"data.yaml\"))\n",
    "\n",
    "if onnx_path and os.path.exists(str(onnx_path)):\n",
    "    shutil.copy2(str(onnx_path), os.path.join(OUTPUT_DIR, \"best.onnx\"))\n",
    "if ts_path and os.path.exists(str(ts_path)):\n",
    "    shutil.copy2(str(ts_path), os.path.join(OUTPUT_DIR, \"best.torchscript\"))\n",
    "\n",
    "# Save class names\n",
    "with open(os.path.join(OUTPUT_DIR, \"classes.json\"), 'w') as f:\n",
    "    json.dump({\"names\": class_names, \"mapping\": class_mapping}, f, indent=2)\n",
    "\n",
    "# Save training config\n",
    "train_config = {\n",
    "    \"model\": MODEL_SIZE, \"imgsz\": IMGSZ, \"epochs\": EPOCHS,\n",
    "    \"batch\": BATCH, \"seed\": SEED, \"split_ratio\": SPLIT_RATIO,\n",
    "    \"num_classes\": num_classes, \"class_names\": class_names,\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"train_config.json\"), 'w') as f:\n",
    "    json.dump(train_config, f, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ“¦ Artifacts saved to: {OUTPUT_DIR}\")\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    size = os.path.getsize(os.path.join(OUTPUT_DIR, f))\n",
    "    print(f\"   {f:30s} {size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3aa2e4",
   "metadata": {},
   "source": [
    "## 10 Â· Package & Download\n",
    "\n",
    "Zip the repository and artifacts for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Zip the repository â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import zipfile\n",
    "\n",
    "def zip_directory(source_dir, output_path):\n",
    "    \"\"\"Create a zip file from a directory.\"\"\"\n",
    "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(source_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, os.path.dirname(source_dir))\n",
    "                zipf.write(file_path, arcname)\n",
    "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    print(f\"   ğŸ“¦ {output_path} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(\"Packaging repository and artifacts...\\n\")\n",
    "\n",
    "# Zip repository\n",
    "repo_zip = \"/content/yolov8_pipeline_repo.zip\"\n",
    "zip_directory(REPO_DIR, repo_zip)\n",
    "\n",
    "# Zip artifacts only\n",
    "artifacts_zip = \"/content/yolov8_trained_artifacts.zip\"\n",
    "zip_directory(OUTPUT_DIR, artifacts_zip)\n",
    "\n",
    "print(f\"\\nâœ… Ready for download!\")\n",
    "print(f\"   ğŸ“¥ Repository:  {repo_zip}\")\n",
    "print(f\"   ğŸ“¥ Artifacts:   {artifacts_zip}\")\n",
    "\n",
    "# Download in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"\\nâ¬‡ï¸  Downloading repository zip...\")\n",
    "    files.download(repo_zip)\n",
    "    print(\"â¬‡ï¸  Downloading artifacts zip...\")\n",
    "    files.download(artifacts_zip)\n",
    "except ImportError:\n",
    "    print(\"\\nğŸ’¡ Not running in Colab. Files are saved at the paths above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Â· Summary\n",
    "\n",
    "### What was created:\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **YOLO Dataset** | Images + labels in train/val splits with `data.yaml` |\n",
    "| **Trained Model** | `best.pt` weights with logged metrics |\n",
    "| **Exports** | ONNX and TorchScript formats |\n",
    "| **Documentation** | README, dataset format guide, usage guide, model card |\n",
    "| **Scripts** | Standalone conversion, training, and prediction scripts |\n",
    "| **Package** | Installable Python package with reusable functions |\n",
    "| **Configs** | YAML configuration for reproducible experiments |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Fine-tune**: Adjust hyperparameters in `configs/default.yaml` and retrain\n",
    "2. **Deploy**: Use the ONNX export for production inference\n",
    "\n",
    "---\n",
    "> ğŸ‰ **Pipeline complete!** Your professional YOLOv8 repository is ready."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
