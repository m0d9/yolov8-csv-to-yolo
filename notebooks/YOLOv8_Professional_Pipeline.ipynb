{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ YOLOv8 Professional Training Pipeline\n",
    "## CSV â†’ YOLO Conversion Â· Training Â· Evaluation Â· Export Â· Repository Builder\n",
    "\n",
    "**Features:**\n",
    "- Auto-detect CSV annotation format (pixel boxes, width/height, normalized YOLO)\n",
    "- Deterministic train/val split with configurable ratio\n",
    "- Full YOLOv8 training with Ultralytics\n",
    "- Evaluation, inference visualization, and model export (ONNX, TorchScript)\n",
    "- **Generates a complete professional repository** for GitHub & Hugging Face\n",
    "\n",
    "---\n",
    "> ğŸ“Œ **How to use:** Run cells sequentially. Edit the **Configuration** cell to match your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Â· Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Install pinned dependencies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "!pip install -q ultralytics==8.2.103 opencv-python-headless==4.10.0.84 \\\n",
    "    pandas==2.2.2 pyyaml==6.0.2 matplotlib==3.9.2 onnx==1.16.2 \\\n",
    "    onnxruntime==1.19.2 Pillow==10.4.0 tqdm==4.66.5 seaborn==0.13.2\n",
    "\n",
    "import ultralytics\n",
    "print(f\"Ultralytics version: {ultralytics.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Â· Configuration\n",
    "\n",
    "Edit the paths and hyperparameters below to match your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONFIGURATION â€” edit these values\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IMAGES_DIR   = \"/content/data/images\"       # folder with jpg/png images\n",
    "CSV_PATH     = \"/content/data/annotations.csv\"  # CSV annotation file\n",
    "WORK_DIR     = \"/content/work\"              # intermediate files\n",
    "OUTPUT_DIR   = \"/content/output\"            # final artifacts\n",
    "REPO_DIR     = \"/content/repo\"              # generated repository\n",
    "\n",
    "# â”€â”€ Train / Val split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SPLIT_RATIO  = 0.8     # fraction for training (rest â†’ validation)\n",
    "SEED         = 42\n",
    "\n",
    "# â”€â”€ Training hyperparameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MODEL_SIZE   = \"yolov8n.pt\"   # yolov8n / yolov8s / yolov8m / yolov8l / yolov8x\n",
    "IMGSZ        = 640\n",
    "EPOCHS       = 50\n",
    "BATCH        = 16\n",
    "DEVICE       = \"\"             # \"\" = auto, \"0\" = GPU 0, \"cpu\" = CPU\n",
    "\n",
    "# â”€â”€ Options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ALLOW_BACKGROUND_IMAGES = True   # keep images with no labels as background\n",
    "LICENSE_TYPE = \"MIT\"              # \"MIT\" or \"Apache-2.0\"\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "print(\"âœ… Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Â· Imports & Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, random, glob, pathlib, warnings, sys, yaml, csv, json\n",
    "from pathlib import Path\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# â”€â”€ Helper: get image dimensions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_image_size(path):\n",
    "    \"\"\"Return (width, height) of an image file.\"\"\"\n",
    "    with Image.open(path) as img:\n",
    "        return img.size  # (w, h)\n",
    "\n",
    "# â”€â”€ Helper: detect CSV format â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def detect_csv_format(df):\n",
    "    \"\"\"\n",
    "    Auto-detect CSV annotation format.\n",
    "    Returns one of: 'A' (xmin,ymin,xmax,ymax), 'B' (xmin,ymin,w,h),\n",
    "                     'C' (normalized YOLO), or raises ValueError.\n",
    "    \"\"\"\n",
    "    cols_lower = [c.lower().strip() for c in df.columns]\n",
    "\n",
    "    # Check for 'normalized' column â†’ Format C\n",
    "    if 'normalized' in cols_lower:\n",
    "        print(\"ğŸ“ Detected Format C (normalized YOLO-like)\")\n",
    "        return 'C'\n",
    "\n",
    "    # Check column names for xmax/ymax â†’ Format A\n",
    "    if 'xmax' in cols_lower and 'ymax' in cols_lower:\n",
    "        print(\"ğŸ“ Detected Format A (xmin, ymin, xmax, ymax)\")\n",
    "        return 'A'\n",
    "\n",
    "    # Check for width/height columns â†’ Format B\n",
    "    if 'width' in cols_lower and 'height' in cols_lower:\n",
    "        print(\"ğŸ“ Detected Format B (xmin, ymin, width, height)\")\n",
    "        return 'B'\n",
    "\n",
    "    # Fallback: try positional detection\n",
    "    if len(df.columns) >= 6:\n",
    "        # Check if values look normalized (all between 0 and 1)\n",
    "        numeric_cols = df.iloc[:, 1:5]\n",
    "        try:\n",
    "            numeric_vals = numeric_cols.astype(float)\n",
    "            if numeric_vals.max().max() <= 1.0 and numeric_vals.min().min() >= 0.0:\n",
    "                print(\"ğŸ“ Auto-detected Format C (values appear normalized)\")\n",
    "                return 'C'\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "\n",
    "        print(\"ğŸ“ Falling back to Format A (positional: col1-4 as xmin,ymin,xmax,ymax)\")\n",
    "        return 'A'\n",
    "\n",
    "    raise ValueError(\n",
    "        \"âŒ Cannot detect CSV format. Expected columns like:\\n\"\n",
    "        \"  Format A: filename, xmin, ymin, xmax, ymax, class\\n\"\n",
    "        \"  Format B: filename, xmin, ymin, width, height, class\\n\"\n",
    "        \"  Format C: filename, x_center, y_center, width, height, class, normalized\"\n",
    "    )\n",
    "\n",
    "# â”€â”€ Helper: normalize columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def normalize_columns(df, fmt):\n",
    "    \"\"\"Rename columns to a standard set based on detected format.\"\"\"\n",
    "    df = df.copy()\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    if fmt == 'A':\n",
    "        mapping = {}\n",
    "        cols_lower = {c.lower().strip(): c for c in cols}\n",
    "        for target, candidates in [\n",
    "            ('filename', ['filename', 'file', 'image', 'image_name', 'img']),\n",
    "            ('xmin', ['xmin', 'x_min', 'x1', 'left']),\n",
    "            ('ymin', ['ymin', 'y_min', 'y1', 'top']),\n",
    "            ('xmax', ['xmax', 'x_max', 'x2', 'right']),\n",
    "            ('ymax', ['ymax', 'y_max', 'y2', 'bottom']),\n",
    "            ('class', ['class', 'label', 'class_name', 'category', 'cls']),\n",
    "        ]:\n",
    "            for c in candidates:\n",
    "                if c in cols_lower:\n",
    "                    mapping[cols_lower[c]] = target\n",
    "                    break\n",
    "        if len(mapping) < 6:\n",
    "            # positional fallback\n",
    "            mapping = {cols[0]: 'filename', cols[1]: 'xmin', cols[2]: 'ymin',\n",
    "                       cols[3]: 'xmax', cols[4]: 'ymax', cols[5]: 'class'}\n",
    "        df = df.rename(columns=mapping)\n",
    "\n",
    "    elif fmt == 'B':\n",
    "        cols_lower = {c.lower().strip(): c for c in cols}\n",
    "        mapping = {}\n",
    "        for target, candidates in [\n",
    "            ('filename', ['filename', 'file', 'image', 'image_name', 'img']),\n",
    "            ('xmin', ['xmin', 'x_min', 'x1', 'left', 'x']),\n",
    "            ('ymin', ['ymin', 'y_min', 'y1', 'top', 'y']),\n",
    "            ('width', ['width', 'w', 'box_width']),\n",
    "            ('height', ['height', 'h', 'box_height']),\n",
    "            ('class', ['class', 'label', 'class_name', 'category', 'cls']),\n",
    "        ]:\n",
    "            for c in candidates:\n",
    "                if c in cols_lower:\n",
    "                    mapping[cols_lower[c]] = target\n",
    "                    break\n",
    "        if len(mapping) < 6:\n",
    "            mapping = {cols[0]: 'filename', cols[1]: 'xmin', cols[2]: 'ymin',\n",
    "                       cols[3]: 'width', cols[4]: 'height', cols[5]: 'class'}\n",
    "        df = df.rename(columns=mapping)\n",
    "\n",
    "    elif fmt == 'C':\n",
    "        cols_lower = {c.lower().strip(): c for c in cols}\n",
    "        mapping = {}\n",
    "        for target, candidates in [\n",
    "            ('filename', ['filename', 'file', 'image', 'image_name', 'img']),\n",
    "            ('x_center', ['x_center', 'cx', 'center_x', 'x']),\n",
    "            ('y_center', ['y_center', 'cy', 'center_y', 'y']),\n",
    "            ('width', ['width', 'w', 'box_width']),\n",
    "            ('height', ['height', 'h', 'box_height']),\n",
    "            ('class', ['class', 'label', 'class_name', 'category', 'cls']),\n",
    "        ]:\n",
    "            for c in candidates:\n",
    "                if c in cols_lower:\n",
    "                    mapping[cols_lower[c]] = target\n",
    "                    break\n",
    "        if len(mapping) < 6:\n",
    "            mapping = {cols[0]: 'filename', cols[1]: 'x_center', cols[2]: 'y_center',\n",
    "                       cols[3]: 'width', cols[4]: 'height', cols[5]: 'class'}\n",
    "        df = df.rename(columns=mapping)\n",
    "\n",
    "    # Normalize filename to basename\n",
    "    df['filename'] = df['filename'].apply(lambda x: os.path.basename(str(x).strip()))\n",
    "    return df\n",
    "\n",
    "# â”€â”€ Helper: convert to YOLO format â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def convert_to_yolo(row, fmt, img_w, img_h):\n",
    "    \"\"\"\n",
    "    Convert a single annotation row to YOLO format:\n",
    "    (x_center, y_center, width, height) all normalized to [0, 1].\n",
    "    Returns None if the box is invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if fmt == 'A':\n",
    "            xmin = float(row['xmin'])\n",
    "            ymin = float(row['ymin'])\n",
    "            xmax = float(row['xmax'])\n",
    "            ymax = float(row['ymax'])\n",
    "        elif fmt == 'B':\n",
    "            xmin = float(row['xmin'])\n",
    "            ymin = float(row['ymin'])\n",
    "            xmax = xmin + float(row['width'])\n",
    "            ymax = ymin + float(row['height'])\n",
    "        elif fmt == 'C':\n",
    "            # Already normalized\n",
    "            xc = float(row['x_center'])\n",
    "            yc = float(row['y_center'])\n",
    "            w  = float(row['width'])\n",
    "            h  = float(row['height'])\n",
    "            # Clamp\n",
    "            xc = max(0.0, min(1.0, xc))\n",
    "            yc = max(0.0, min(1.0, yc))\n",
    "            w  = max(0.0, min(1.0, w))\n",
    "            h  = max(0.0, min(1.0, h))\n",
    "            if w <= 0 or h <= 0:\n",
    "                return None\n",
    "            return (xc, yc, w, h)\n",
    "\n",
    "        # For A and B: clamp to image bounds\n",
    "        xmin = max(0, min(img_w, xmin))\n",
    "        ymin = max(0, min(img_h, ymin))\n",
    "        xmax = max(0, min(img_w, xmax))\n",
    "        ymax = max(0, min(img_h, ymax))\n",
    "\n",
    "        bw = xmax - xmin\n",
    "        bh = ymax - ymin\n",
    "        if bw <= 0 or bh <= 0:\n",
    "            return None\n",
    "\n",
    "        xc = (xmin + xmax) / 2.0 / img_w\n",
    "        yc = (ymin + ymax) / 2.0 / img_h\n",
    "        w  = bw / img_w\n",
    "        h  = bh / img_h\n",
    "        return (xc, yc, w, h)\n",
    "\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "# â”€â”€ Helper: build class mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_class_mapping(classes_series):\n",
    "    \"\"\"\n",
    "    Build a mapping from class label to integer ID.\n",
    "    If labels are already integers, validate contiguous from 0; remap if needed.\n",
    "    Returns: dict {label: int_id}, list of class names ordered by id.\n",
    "    \"\"\"\n",
    "    unique_classes = classes_series.unique()\n",
    "\n",
    "    # Check if all are integers\n",
    "    all_int = True\n",
    "    for c in unique_classes:\n",
    "        try:\n",
    "            int(c)\n",
    "        except (ValueError, TypeError):\n",
    "            all_int = False\n",
    "            break\n",
    "\n",
    "    if all_int:\n",
    "        int_classes = sorted([int(c) for c in unique_classes])\n",
    "        if int_classes == list(range(len(int_classes))):\n",
    "            mapping = {str(c): c for c in int_classes}\n",
    "            names = [str(c) for c in int_classes]\n",
    "        else:\n",
    "            # Remap\n",
    "            mapping = {str(c): i for i, c in enumerate(int_classes)}\n",
    "            names = [str(c) for c in int_classes]\n",
    "            print(f\"âš ï¸  Class IDs are not contiguous. Remapped: {mapping}\")\n",
    "    else:\n",
    "        sorted_classes = sorted([str(c) for c in unique_classes])\n",
    "        mapping = {c: i for i, c in enumerate(sorted_classes)}\n",
    "        names = sorted_classes\n",
    "\n",
    "    return mapping, names\n",
    "\n",
    "# â”€â”€ Helper: visualize boxes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def visualize_yolo_labels(image_path, label_path, class_names, ax=None):\n",
    "    \"\"\"Draw YOLO bounding boxes on an image.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    w, h = img.size\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(os.path.basename(image_path), fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, max(len(class_names), 1)))\n",
    "\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                cls_id = int(parts[0])\n",
    "                xc, yc, bw, bh = [float(x) for x in parts[1:]]\n",
    "                x1 = (xc - bw / 2) * w\n",
    "                y1 = (yc - bh / 2) * h\n",
    "                rect_w = bw * w\n",
    "                rect_h = bh * h\n",
    "                color = colors[cls_id % len(colors)]\n",
    "                rect = patches.Rectangle((x1, y1), rect_w, rect_h,\n",
    "                                         linewidth=2, edgecolor=color,\n",
    "                                         facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                label = class_names[cls_id] if cls_id < len(class_names) else str(cls_id)\n",
    "                ax.text(x1, y1 - 4, label, fontsize=8, color='white',\n",
    "                        bbox=dict(boxstyle='round,pad=0.2', facecolor=color, alpha=0.8))\n",
    "    return ax\n",
    "\n",
    "print(\"âœ… Utility functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Â· Data Validation & Inspection\n",
    "\n",
    "Read the CSV, auto-detect format, validate data, and display summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Read CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"ğŸ“„ CSV loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
    "print(f\"   Columns: {list(df.columns)}\")\n",
    "print()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Detect format & normalize columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "csv_format = detect_csv_format(df)\n",
    "df = normalize_columns(df, csv_format)\n",
    "print(f\"\\nğŸ“Š Format: {csv_format}\")\n",
    "print(f\"   Unique classes: {df['class'].nunique()}\")\n",
    "print(f\"   Class distribution:\")\n",
    "print(df['class'].value_counts().to_string())\n",
    "print(f\"\\n   Unique images in CSV: {df['filename'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Validate image files exist â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}\n",
    "available_images = {}\n",
    "for f in os.listdir(IMAGES_DIR):\n",
    "    if pathlib.Path(f).suffix.lower() in image_extensions:\n",
    "        available_images[f] = os.path.join(IMAGES_DIR, f)\n",
    "\n",
    "csv_filenames = set(df['filename'].unique())\n",
    "found = csv_filenames & set(available_images.keys())\n",
    "missing = csv_filenames - set(available_images.keys())\n",
    "\n",
    "print(f\"ğŸ–¼ï¸  Images in folder: {len(available_images)}\")\n",
    "print(f\"   Matched to CSV:   {len(found)}\")\n",
    "if missing:\n",
    "    print(f\"   âš ï¸  Missing files:  {len(missing)}\")\n",
    "    for m in list(missing)[:10]:\n",
    "        print(f\"      - {m}\")\n",
    "    if len(missing) > 10:\n",
    "        print(f\"      ... and {len(missing) - 10} more\")\n",
    "    # Remove rows with missing images\n",
    "    df = df[df['filename'].isin(found)]\n",
    "    print(f\"   â†’ Kept {len(df)} rows after removing missing images.\")\n",
    "\n",
    "assert len(found) > 0, \"âŒ No matching images found. Check IMAGES_DIR and CSV filenames.\"\n",
    "print(\"\\nâœ… Data validation passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Validate & clean bounding boxes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "initial_count = len(df)\n",
    "clamped_count = 0\n",
    "\n",
    "if csv_format in ('A', 'B'):\n",
    "    # Check for NaN in coordinate columns\n",
    "    coord_cols = ['xmin', 'ymin']\n",
    "    coord_cols += ['xmax', 'ymax'] if csv_format == 'A' else ['width', 'height']\n",
    "    df = df.dropna(subset=coord_cols + ['class', 'filename'])\n",
    "\n",
    "    # Convert to numeric\n",
    "    for col in coord_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df = df.dropna(subset=coord_cols)\n",
    "\n",
    "elif csv_format == 'C':\n",
    "    coord_cols = ['x_center', 'y_center', 'width', 'height']\n",
    "    df = df.dropna(subset=coord_cols + ['class', 'filename'])\n",
    "    for col in coord_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df = df.dropna(subset=coord_cols)\n",
    "\n",
    "dropped = initial_count - len(df)\n",
    "if dropped > 0:\n",
    "    print(f\"âš ï¸  Dropped {dropped} rows with NaN/invalid values.\")\n",
    "\n",
    "print(f\"âœ… {len(df)} valid annotation rows remaining.\")\n",
    "print(f\"   Covering {df['filename'].nunique()} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Â· YOLO Dataset Conversion Pipeline\n",
    "\n",
    "Create the YOLO directory structure, split into train/val, convert annotations, and generate `data.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Build class mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class_mapping, class_names = build_class_mapping(df['class'])\n",
    "num_classes = len(class_names)\n",
    "print(f\"ğŸ·ï¸  {num_classes} classes: {class_names}\")\n",
    "print(f\"   Mapping: {class_mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Create YOLO directory structure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DATASET_DIR = os.path.join(WORK_DIR, \"dataset\")\n",
    "for split in ['train', 'val']:\n",
    "    os.makedirs(os.path.join(DATASET_DIR, 'images', split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(DATASET_DIR, 'labels', split), exist_ok=True)\n",
    "\n",
    "# â”€â”€ Split images into train / val â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_images = sorted(df['filename'].unique())\n",
    "random.seed(SEED)\n",
    "random.shuffle(all_images)\n",
    "split_idx = int(len(all_images) * SPLIT_RATIO)\n",
    "train_images = set(all_images[:split_idx])\n",
    "val_images   = set(all_images[split_idx:])\n",
    "\n",
    "# Handle background images (images in folder but not in CSV)\n",
    "if ALLOW_BACKGROUND_IMAGES:\n",
    "    bg_images = set(available_images.keys()) - csv_filenames\n",
    "    if bg_images:\n",
    "        bg_list = sorted(bg_images)\n",
    "        random.shuffle(bg_list)\n",
    "        bg_split = int(len(bg_list) * SPLIT_RATIO)\n",
    "        train_images |= set(bg_list[:bg_split])\n",
    "        val_images   |= set(bg_list[bg_split:])\n",
    "        print(f\"ğŸ–¼ï¸  Added {len(bg_images)} background images (no labels).\")\n",
    "\n",
    "print(f\"ğŸ“‚ Train: {len(train_images)} images | Val: {len(val_images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Convert annotations & copy images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "stats = {'train': 0, 'val': 0, 'boxes': 0, 'skipped_boxes': 0}\n",
    "\n",
    "for img_name in tqdm(sorted(train_images | val_images), desc=\"Converting\"):\n",
    "    split = 'train' if img_name in train_images else 'val'\n",
    "    stats[split] += 1\n",
    "\n",
    "    # Copy image\n",
    "    src_path = available_images.get(img_name)\n",
    "    if src_path is None:\n",
    "        continue\n",
    "    dst_img = os.path.join(DATASET_DIR, 'images', split, img_name)\n",
    "    if not os.path.exists(dst_img):\n",
    "        shutil.copy2(src_path, dst_img)\n",
    "\n",
    "    # Get image dimensions\n",
    "    img_w, img_h = get_image_size(src_path)\n",
    "\n",
    "    # Get annotations for this image\n",
    "    img_df = df[df['filename'] == img_name]\n",
    "\n",
    "    # Write label file\n",
    "    label_name = os.path.splitext(img_name)[0] + '.txt'\n",
    "    label_path = os.path.join(DATASET_DIR, 'labels', split, label_name)\n",
    "\n",
    "    lines = []\n",
    "    for _, row in img_df.iterrows():\n",
    "        result = convert_to_yolo(row, csv_format, img_w, img_h)\n",
    "        if result is None:\n",
    "            stats['skipped_boxes'] += 1\n",
    "            continue\n",
    "        xc, yc, w, h = result\n",
    "        cls_id = class_mapping[str(row['class'])]\n",
    "        lines.append(f\"{cls_id} {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\")\n",
    "        stats['boxes'] += 1\n",
    "\n",
    "    with open(label_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(f\"\\nâœ… Conversion complete!\")\n",
    "print(f\"   Train images: {stats['train']}\")\n",
    "print(f\"   Val images:   {stats['val']}\")\n",
    "print(f\"   Total boxes:  {stats['boxes']}\")\n",
    "if stats['skipped_boxes'] > 0:\n",
    "    print(f\"   âš ï¸  Skipped boxes: {stats['skipped_boxes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Generate data.yaml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "data_yaml = {\n",
    "    'path': DATASET_DIR,\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'nc': num_classes,\n",
    "    'names': class_names,\n",
    "}\n",
    "\n",
    "data_yaml_path = os.path.join(DATASET_DIR, 'data.yaml')\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    yaml.dump(data_yaml, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"ğŸ“„ data.yaml written to: {data_yaml_path}\")\n",
    "print(\"â”€\" * 50)\n",
    "with open(data_yaml_path) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Â· Visualization\n",
    "\n",
    "Display sample training images with bounding boxes to verify the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Visualize sample training images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_img_dir = os.path.join(DATASET_DIR, 'images', 'train')\n",
    "train_lbl_dir = os.path.join(DATASET_DIR, 'labels', 'train')\n",
    "\n",
    "sample_images = [f for f in os.listdir(train_img_dir)\n",
    "                 if pathlib.Path(f).suffix.lower() in image_extensions]\n",
    "\n",
    "# Prefer images that have labels\n",
    "labeled = [f for f in sample_images\n",
    "           if os.path.getsize(os.path.join(train_lbl_dir,\n",
    "              os.path.splitext(f)[0] + '.txt')) > 0\n",
    "           if os.path.exists(os.path.join(train_lbl_dir,\n",
    "              os.path.splitext(f)[0] + '.txt'))]\n",
    "\n",
    "display_images = labeled[:5] if len(labeled) >= 5 else sample_images[:5]\n",
    "n = len(display_images)\n",
    "\n",
    "fig, axes = plt.subplots(1, n, figsize=(5 * n, 5))\n",
    "if n == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, img_name in zip(axes, display_images):\n",
    "    img_path = os.path.join(train_img_dir, img_name)\n",
    "    lbl_path = os.path.join(train_lbl_dir, os.path.splitext(img_name)[0] + '.txt')\n",
    "    visualize_yolo_labels(img_path, lbl_path, class_names, ax=ax)\n",
    "\n",
    "plt.suptitle(\"Sample Training Images with YOLO Bounding Boxes\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Â· YOLOv8 Training\n",
    "\n",
    "Train the model using Ultralytics. Metrics are logged automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# â”€â”€ Initialize model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model = YOLO(MODEL_SIZE)\n",
    "print(f\"ğŸ”§ Model: {MODEL_SIZE}\")\n",
    "print(f\"   Image size: {IMGSZ}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Batch: {BATCH}\")\n",
    "print(f\"   Device: {DEVICE if DEVICE else 'auto'}\")\n",
    "\n",
    "# â”€â”€ Train â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "results = model.train(\n",
    "    data=data_yaml_path,\n",
    "    imgsz=IMGSZ,\n",
    "    epochs=EPOCHS,\n",
    "    batch=BATCH,\n",
    "    device=DEVICE if DEVICE else None,\n",
    "    seed=SEED,\n",
    "    project=os.path.join(WORK_DIR, \"runs\"),\n",
    "    name=\"train\",\n",
    "    exist_ok=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Display training metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_dir = os.path.join(WORK_DIR, \"runs\", \"train\")\n",
    "\n",
    "# Show results plots if available\n",
    "for plot_name in ['results.png', 'confusion_matrix.png', 'confusion_matrix_normalized.png']:\n",
    "    plot_path = os.path.join(train_dir, plot_name)\n",
    "    if os.path.exists(plot_path):\n",
    "        img = Image.open(plot_path)\n",
    "        fig, ax = plt.subplots(figsize=(12, 8) if 'results' in plot_name else (8, 8))\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(plot_name, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Â· Evaluation & Inference\n",
    "\n",
    "Run validation and display predictions on sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_weights = os.path.join(train_dir, \"weights\", \"best.pt\")\n",
    "model_best = YOLO(best_weights)\n",
    "\n",
    "val_results = model_best.val(data=data_yaml_path, imgsz=IMGSZ, verbose=True)\n",
    "\n",
    "print(\"\\nğŸ“Š Validation Results:\")\n",
    "print(f\"   mAP50:    {val_results.box.map50:.4f}\")\n",
    "print(f\"   mAP50-95: {val_results.box.map:.4f}\")\n",
    "print(f\"   Precision: {val_results.box.mp:.4f}\")\n",
    "print(f\"   Recall:    {val_results.box.mr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Inference on sample validation images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "val_img_dir = os.path.join(DATASET_DIR, 'images', 'val')\n",
    "val_samples = [f for f in os.listdir(val_img_dir)\n",
    "               if pathlib.Path(f).suffix.lower() in image_extensions][:4]\n",
    "\n",
    "if val_samples:\n",
    "    fig, axes = plt.subplots(1, len(val_samples), figsize=(5 * len(val_samples), 5))\n",
    "    if len(val_samples) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, img_name in zip(axes, val_samples):\n",
    "        img_path = os.path.join(val_img_dir, img_name)\n",
    "        preds = model_best.predict(img_path, imgsz=IMGSZ, conf=0.25, verbose=False)\n",
    "        # Plot using ultralytics built-in\n",
    "        result_img = preds[0].plot()\n",
    "        ax.imshow(result_img[..., ::-1])  # BGR to RGB\n",
    "        ax.set_title(img_name, fontsize=9)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.suptitle(\"Predictions on Validation Images\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸  No validation images found for inference preview.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Â· Model Export\n",
    "\n",
    "Export the best model to ONNX and TorchScript formats, and save all artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Export to ONNX â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    onnx_path = model_best.export(format=\"onnx\", imgsz=IMGSZ)\n",
    "    print(f\"âœ… ONNX exported: {onnx_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  ONNX export failed: {e}\")\n",
    "    onnx_path = None\n",
    "\n",
    "# â”€â”€ Export to TorchScript â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    ts_path = model_best.export(format=\"torchscript\", imgsz=IMGSZ)\n",
    "    print(f\"âœ… TorchScript exported: {ts_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  TorchScript export failed: {e}\")\n",
    "    ts_path = None\n",
    "\n",
    "# â”€â”€ Copy artifacts to output directory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "shutil.copy2(best_weights, os.path.join(OUTPUT_DIR, \"best.pt\"))\n",
    "shutil.copy2(data_yaml_path, os.path.join(OUTPUT_DIR, \"data.yaml\"))\n",
    "\n",
    "if onnx_path and os.path.exists(str(onnx_path)):\n",
    "    shutil.copy2(str(onnx_path), os.path.join(OUTPUT_DIR, \"best.onnx\"))\n",
    "if ts_path and os.path.exists(str(ts_path)):\n",
    "    shutil.copy2(str(ts_path), os.path.join(OUTPUT_DIR, \"best.torchscript\"))\n",
    "\n",
    "# Save class names\n",
    "with open(os.path.join(OUTPUT_DIR, \"classes.json\"), 'w') as f:\n",
    "    json.dump({\"names\": class_names, \"mapping\": class_mapping}, f, indent=2)\n",
    "\n",
    "# Save training config\n",
    "train_config = {\n",
    "    \"model\": MODEL_SIZE, \"imgsz\": IMGSZ, \"epochs\": EPOCHS,\n",
    "    \"batch\": BATCH, \"seed\": SEED, \"split_ratio\": SPLIT_RATIO,\n",
    "    \"num_classes\": num_classes, \"class_names\": class_names,\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"train_config.json\"), 'w') as f:\n",
    "    json.dump(train_config, f, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ“¦ Artifacts saved to: {OUTPUT_DIR}\")\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    size = os.path.getsize(os.path.join(OUTPUT_DIR, f))\n",
    "    print(f\"   {f:30s} {size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Â· Professional Repository Generation\n",
    "\n",
    "Generate a complete, ready-to-use repository structure for **GitHub** and **Hugging Face**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# REPOSITORY STRUCTURE GENERATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import textwrap as _tw\n",
    "\n",
    "# â”€â”€ Create directory structure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "repo_dirs = [\n",
    "    REPO_DIR,\n",
    "    f\"{REPO_DIR}/src/yolov8_pipeline\",\n",
    "    f\"{REPO_DIR}/scripts\",\n",
    "    f\"{REPO_DIR}/configs\",\n",
    "    f\"{REPO_DIR}/docs\",\n",
    "    f\"{REPO_DIR}/artifacts\",\n",
    "    f\"{REPO_DIR}/assets\",\n",
    "    f\"{REPO_DIR}/huggingface\",\n",
    "]\n",
    "for d in repo_dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“ Repository directory structure created:\")\n",
    "for d in repo_dirs:\n",
    "    rel = os.path.relpath(d, REPO_DIR)\n",
    "    depth = rel.count(os.sep)\n",
    "    print(f\"   {'  ' * depth}ğŸ“‚ {os.path.basename(d)}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ README.md â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "readme_content = f\"\"\"# ğŸš€ YOLOv8 Object Detection Pipeline\n",
    "\n",
    "A professional, end-to-end pipeline for training **YOLOv8** object detection models using CSV-formatted annotations. Supports multiple annotation formats with automatic detection, deterministic train/val splitting, and comprehensive model evaluation.\n",
    "\n",
    "![Python](https://img.shields.io/badge/Python-3.8%2B-blue)\n",
    "![Ultralytics](https://img.shields.io/badge/Ultralytics-8.2-purple)\n",
    "![License](https://img.shields.io/badge/License-{LICENSE_TYPE}-green)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Table of Contents\n",
    "\n",
    "- [Features](#-features)\n",
    "- [Quick Start](#-quick-start)\n",
    "- [Data Format](#-data-format)\n",
    "- [Project Structure](#-project-structure)\n",
    "- [Training](#-training)\n",
    "- [Evaluation](#-evaluation)\n",
    "- [Inference](#-inference)\n",
    "- [Export](#-export)\n",
    "- [Configuration](#-configuration)\n",
    "- [Troubleshooting](#-troubleshooting)\n",
    "- [License](#-license)\n",
    "\n",
    "---\n",
    "\n",
    "## âœ¨ Features\n",
    "\n",
    "- **Auto-detect CSV format**: Supports pixel boxes (xmin/ymin/xmax/ymax), width/height format, and normalized YOLO format\n",
    "- **Robust data validation**: Checks for missing images, invalid boxes, NaN values, and out-of-bound coordinates\n",
    "- **Deterministic splitting**: Reproducible train/val splits with configurable seed\n",
    "- **Background image support**: Optionally include unlabeled images as background samples\n",
    "- **Multi-format export**: ONNX, TorchScript, and PyTorch weights\n",
    "- **Professional structure**: Ready for GitHub and Hugging Face deployment\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Quick Start\n",
    "\n",
    "### Option 1: Google Colab (Recommended)\n",
    "\n",
    "Open the included notebook and run all cells:\n",
    "\n",
    "```\n",
    "YOLOv8_Professional_Pipeline.ipynb\n",
    "```\n",
    "\n",
    "### Option 2: Command Line\n",
    "\n",
    "```bash\n",
    "# 1. Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 2. Convert CSV annotations to YOLO format\n",
    "python scripts/convert_csv_to_yolo.py --config configs/default.yaml\n",
    "\n",
    "# 3. Train the model\n",
    "python scripts/train.py --config configs/default.yaml\n",
    "\n",
    "# 4. Run predictions\n",
    "python scripts/predict.py --weights artifacts/best.pt --source path/to/images\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Data Format\n",
    "\n",
    "Place your data in the following structure:\n",
    "\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ images/          # Your image files (jpg, png, etc.)\n",
    "â”‚   â”œâ”€â”€ img001.jpg\n",
    "â”‚   â”œâ”€â”€ img002.jpg\n",
    "â”‚   â””â”€â”€ ...\n",
    "â””â”€â”€ annotations.csv  # Bounding box annotations\n",
    "```\n",
    "\n",
    "### Supported CSV Formats\n",
    "\n",
    "| Format | Columns | Description |\n",
    "|--------|---------|-------------|\n",
    "| **A** | `filename, xmin, ymin, xmax, ymax, class` | Pixel coordinates (corners) |\n",
    "| **B** | `filename, xmin, ymin, width, height, class` | Pixel coordinates (top-left + size) |\n",
    "| **C** | `filename, x_center, y_center, width, height, class, normalized` | Normalized YOLO format |\n",
    "\n",
    "> The format is **auto-detected** from column names. See `docs/dataset_format.md` for details.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Project Structure\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ README.md                    # This file\n",
    "â”œâ”€â”€ requirements.txt             # Python dependencies\n",
    "â”œâ”€â”€ pyproject.toml               # Project metadata\n",
    "â”œâ”€â”€ Makefile                     # Common commands\n",
    "â”œâ”€â”€ .gitignore                   # Git ignore rules\n",
    "â”œâ”€â”€ LICENSE                      # License file\n",
    "â”œâ”€â”€ configs/\n",
    "â”‚   â””â”€â”€ default.yaml             # Default configuration\n",
    "â”œâ”€â”€ src/yolov8_pipeline/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ parser.py                # CSV parsing & format detection\n",
    "â”‚   â”œâ”€â”€ converter.py             # YOLO format conversion\n",
    "â”‚   â””â”€â”€ visualizer.py            # Visualization utilities\n",
    "â”œâ”€â”€ scripts/\n",
    "â”‚   â”œâ”€â”€ convert_csv_to_yolo.py   # CSV â†’ YOLO conversion script\n",
    "â”‚   â”œâ”€â”€ train.py                 # Training script\n",
    "â”‚   â””â”€â”€ predict.py               # Inference script\n",
    "â”œâ”€â”€ docs/\n",
    "â”‚   â”œâ”€â”€ dataset_format.md        # Dataset format guide\n",
    "â”‚   â””â”€â”€ usage_guide.md           # Detailed usage guide\n",
    "â”œâ”€â”€ artifacts/                   # Trained model weights & configs\n",
    "â”‚   â”œâ”€â”€ best.pt\n",
    "â”‚   â”œâ”€â”€ data.yaml\n",
    "â”‚   â””â”€â”€ classes.json\n",
    "â”œâ”€â”€ assets/                      # Sample images\n",
    "â””â”€â”€ huggingface/\n",
    "    â””â”€â”€ README.md                # Hugging Face model card\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‹ï¸ Training\n",
    "\n",
    "### Using the Config File\n",
    "\n",
    "```bash\n",
    "python scripts/train.py --config configs/default.yaml\n",
    "```\n",
    "\n",
    "### Using Command Line Arguments\n",
    "\n",
    "```bash\n",
    "python scripts/train.py \\\\\n",
    "    --data path/to/data.yaml \\\\\n",
    "    --model yolov8n.pt \\\\\n",
    "    --imgsz 640 \\\\\n",
    "    --epochs 50 \\\\\n",
    "    --batch 16\n",
    "```\n",
    "\n",
    "### Training Parameters\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `model` | `yolov8n.pt` | Model size (n/s/m/l/x) |\n",
    "| `imgsz` | `640` | Input image size |\n",
    "| `epochs` | `50` | Number of training epochs |\n",
    "| `batch` | `16` | Batch size |\n",
    "| `seed` | `42` | Random seed |\n",
    "| `device` | `auto` | Device (cuda/cpu) |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ Evaluation\n",
    "\n",
    "```bash\n",
    "python -c \"from ultralytics import YOLO; m = YOLO('artifacts/best.pt'); m.val(data='artifacts/data.yaml')\"\n",
    "```\n",
    "\n",
    "Key metrics reported:\n",
    "- **mAP50**: Mean Average Precision at IoU 0.50\n",
    "- **mAP50-95**: Mean Average Precision at IoU 0.50-0.95\n",
    "- **Precision**: Detection precision\n",
    "- **Recall**: Detection recall\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Inference\n",
    "\n",
    "```bash\n",
    "python scripts/predict.py \\\\\n",
    "    --weights artifacts/best.pt \\\\\n",
    "    --source path/to/images \\\\\n",
    "    --conf 0.25 \\\\\n",
    "    --imgsz 640\n",
    "```\n",
    "\n",
    "### Python API\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"artifacts/best.pt\")\n",
    "results = model.predict(\"image.jpg\", conf=0.25)\n",
    "\n",
    "for r in results:\n",
    "    boxes = r.boxes\n",
    "    for box in boxes:\n",
    "        print(f\"Class: {{box.cls}}, Confidence: {{box.conf}}, BBox: {{box.xyxy}}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ Export\n",
    "\n",
    "The model can be exported to multiple formats:\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "model = YOLO(\"artifacts/best.pt\")\n",
    "\n",
    "model.export(format=\"onnx\")        # ONNX format\n",
    "model.export(format=\"torchscript\") # TorchScript\n",
    "model.export(format=\"tflite\")      # TensorFlow Lite\n",
    "model.export(format=\"engine\")      # TensorRT\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Configuration\n",
    "\n",
    "Edit `configs/default.yaml` to customize all parameters:\n",
    "\n",
    "```yaml\n",
    "paths:\n",
    "  images_dir: data/images\n",
    "  csv_path: data/annotations.csv\n",
    "  output_dir: artifacts\n",
    "\n",
    "training:\n",
    "  model: yolov8n.pt\n",
    "  imgsz: 640\n",
    "  epochs: 50\n",
    "  batch: 16\n",
    "  seed: 42\n",
    "\n",
    "data:\n",
    "  split_ratio: 0.8\n",
    "  allow_background: true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Troubleshooting\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| `No matching images found` | Check that `images_dir` points to the correct folder and filenames in CSV match |\n",
    "| `Cannot detect CSV format` | Ensure CSV has proper column headers (see Data Format section) |\n",
    "| `CUDA out of memory` | Reduce `batch` size or use a smaller model (yolov8n) |\n",
    "| `Invalid bounding boxes` | Check that coordinates are within image dimensions |\n",
    "| `Low mAP scores` | Increase epochs, try larger model, or check annotation quality |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“„ License\n",
    "\n",
    "This project is licensed under the {LICENSE_TYPE} License. See the [LICENSE](LICENSE) file for details.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤ Acknowledgments\n",
    "\n",
    "- [Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics)\n",
    "- [YOLO Documentation](https://docs.ultralytics.com/)\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(REPO_DIR, \"README.md\"), 'w') as f:\n",
    "    f.write(readme_content)\n",
    "print(\"âœ… README.md generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ requirements.txt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "requirements = \"\"\"ultralytics==8.2.103\n",
    "opencv-python-headless==4.10.0.84\n",
    "pandas==2.2.2\n",
    "pyyaml==6.0.2\n",
    "matplotlib==3.9.2\n",
    "Pillow==10.4.0\n",
    "tqdm==4.66.5\n",
    "seaborn==0.13.2\n",
    "onnx==1.16.2\n",
    "onnxruntime==1.19.2\n",
    "\"\"\"\n",
    "with open(os.path.join(REPO_DIR, \"requirements.txt\"), 'w') as f:\n",
    "    f.write(requirements.strip() + \"\\n\")\n",
    "print(\"âœ… requirements.txt\")\n",
    "\n",
    "# â”€â”€ .gitignore â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "gitignore = \"\"\"# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*.egg-info/\n",
    "dist/\n",
    "build/\n",
    "*.egg\n",
    ".eggs/\n",
    "\n",
    "# Virtual environments\n",
    "venv/\n",
    "env/\n",
    ".env\n",
    "\n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "*.swp\n",
    "*.swo\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# Data & Models (large files)\n",
    "*.pt\n",
    "*.onnx\n",
    "*.torchscript\n",
    "*.tflite\n",
    "*.engine\n",
    "\n",
    "# Dataset (user data)\n",
    "data/images/\n",
    "data/annotations.csv\n",
    "dataset/\n",
    "\n",
    "# Training runs\n",
    "runs/\n",
    "wandb/\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints/\n",
    "\"\"\"\n",
    "with open(os.path.join(REPO_DIR, \".gitignore\"), 'w') as f:\n",
    "    f.write(gitignore.strip() + \"\\n\")\n",
    "print(\"âœ… .gitignore\")\n",
    "\n",
    "# â”€â”€ LICENSE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if LICENSE_TYPE == \"MIT\":\n",
    "    license_text = \"\"\"MIT License\n",
    "\n",
    "Copyright (c) 2024\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\"\"\"\n",
    "else:\n",
    "    license_text = \"\"\"                                 Apache License\n",
    "                           Version 2.0, January 2004\n",
    "                        http://www.apache.org/licenses/\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "\"\"\"\n",
    "with open(os.path.join(REPO_DIR, \"LICENSE\"), 'w') as f:\n",
    "    f.write(license_text.strip() + \"\\n\")\n",
    "print(\"âœ… LICENSE\")\n",
    "\n",
    "# â”€â”€ pyproject.toml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pyproject = \"\"\"[build-system]\n",
    "requires = [\"setuptools>=68.0\", \"wheel\"]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "\n",
    "[project]\n",
    "name = \"yolov8-pipeline\"\n",
    "version = \"1.0.0\"\n",
    "description = \"Professional YOLOv8 object detection training pipeline with CSV annotation support\"\n",
    "readme = \"README.md\"\n",
    "requires-python = \">=3.8\"\n",
    "license = {text = \"\"\"\" + LICENSE_TYPE + \"\"\"\"}\n",
    "authors = [{name = \"YOLOv8 Pipeline Team\"}]\n",
    "keywords = [\"yolov8\", \"object-detection\", \"computer-vision\", \"deep-learning\"]\n",
    "classifiers = [\n",
    "    \"Development Status :: 4 - Beta\",\n",
    "    \"Intended Audience :: Developers\",\n",
    "    \"Intended Audience :: Science/Research\",\n",
    "    \"Programming Language :: Python :: 3\",\n",
    "    \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n",
    "    \"Topic :: Scientific/Engineering :: Image Recognition\",\n",
    "]\n",
    "dependencies = [\n",
    "    \"ultralytics>=8.2\",\n",
    "    \"opencv-python-headless>=4.8\",\n",
    "    \"pandas>=2.0\",\n",
    "    \"pyyaml>=6.0\",\n",
    "    \"matplotlib>=3.7\",\n",
    "    \"Pillow>=10.0\",\n",
    "    \"tqdm>=4.65\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "export = [\"onnx>=1.14\", \"onnxruntime>=1.16\"]\n",
    "dev = [\"pytest\", \"black\", \"ruff\", \"pre-commit\"]\n",
    "\n",
    "[tool.setuptools.packages.find]\n",
    "where = [\"src\"]\n",
    "\n",
    "[tool.black]\n",
    "line-length = 100\n",
    "\n",
    "[tool.ruff]\n",
    "line-length = 100\n",
    "target-version = \"py38\"\n",
    "\"\"\"\n",
    "with open(os.path.join(REPO_DIR, \"pyproject.toml\"), 'w') as f:\n",
    "    f.write(pyproject.strip() + \"\\n\")\n",
    "print(\"âœ… pyproject.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Makefile â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "makefile = \"\"\"# YOLOv8 Pipeline Makefile\n",
    ".PHONY: install convert train predict export clean help\n",
    "\n",
    "help: ## Show this help message\n",
    "\\t@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | \\\\\n",
    "\\t\\tawk 'BEGIN {FS = \":.*?## \"}; {printf \"\\\\033[36m%-20s\\\\033[0m %s\\\\n\", $$1, $$2}'\n",
    "\n",
    "install: ## Install dependencies\n",
    "\\tpip install -r requirements.txt\n",
    "\n",
    "convert: ## Convert CSV annotations to YOLO format\n",
    "\\tpython scripts/convert_csv_to_yolo.py --config configs/default.yaml\n",
    "\n",
    "train: ## Train YOLOv8 model\n",
    "\\tpython scripts/train.py --config configs/default.yaml\n",
    "\n",
    "predict: ## Run inference on images\n",
    "\\tpython scripts/predict.py --weights artifacts/best.pt --source data/images\n",
    "\n",
    "export: ## Export model to ONNX\n",
    "\\tpython -c \"from ultralytics import YOLO; YOLO('artifacts/best.pt').export(format='onnx')\"\n",
    "\n",
    "clean: ## Clean generated files\n",
    "\\trm -rf runs/ dataset/ __pycache__/\n",
    "\\tfind . -type d -name __pycache__ -exec rm -rf {} + 2>/dev/null || true\n",
    "\n",
    "lint: ## Run linter\n",
    "\\truff check src/ scripts/\n",
    "\n",
    "format: ## Format code\n",
    "\\tblack src/ scripts/\n",
    "\"\"\"\n",
    "with open(os.path.join(REPO_DIR, \"Makefile\"), 'w') as f:\n",
    "    f.write(makefile)\n",
    "print(\"âœ… Makefile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ src/yolov8_pipeline/__init__.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "init_py = '''\"\"\"YOLOv8 Pipeline - Professional object detection training pipeline.\"\"\"\n",
    "\n",
    "__version__ = \"1.0.0\"\n",
    "\n",
    "from .parser import detect_csv_format, normalize_columns, build_class_mapping\n",
    "from .converter import convert_to_yolo, create_yolo_dataset\n",
    "from .visualizer import visualize_yolo_labels\n",
    "\n",
    "__all__ = [\n",
    "    \"detect_csv_format\",\n",
    "    \"normalize_columns\",\n",
    "    \"build_class_mapping\",\n",
    "    \"convert_to_yolo\",\n",
    "    \"create_yolo_dataset\",\n",
    "    \"visualize_yolo_labels\",\n",
    "]\n",
    "'''\n",
    "with open(os.path.join(REPO_DIR, \"src/yolov8_pipeline/__init__.py\"), 'w') as f:\n",
    "    f.write(init_py)\n",
    "\n",
    "# â”€â”€ src/yolov8_pipeline/parser.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "parser_py = '''\"\"\"CSV annotation parsing and format detection.\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def detect_csv_format(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Auto-detect CSV annotation format.\n",
    "\n",
    "    Returns:\n",
    "        'A' - pixel boxes (xmin, ymin, xmax, ymax)\n",
    "        'B' - pixel boxes (xmin, ymin, width, height)\n",
    "        'C' - normalized YOLO format\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If format cannot be determined.\n",
    "    \"\"\"\n",
    "    cols_lower = [c.lower().strip() for c in df.columns]\n",
    "\n",
    "    if \"normalized\" in cols_lower:\n",
    "        return \"C\"\n",
    "    if \"xmax\" in cols_lower and \"ymax\" in cols_lower:\n",
    "        return \"A\"\n",
    "    if \"width\" in cols_lower and \"height\" in cols_lower:\n",
    "        return \"B\"\n",
    "\n",
    "    if len(df.columns) >= 6:\n",
    "        numeric_cols = df.iloc[:, 1:5]\n",
    "        try:\n",
    "            numeric_vals = numeric_cols.astype(float)\n",
    "            if numeric_vals.max().max() <= 1.0 and numeric_vals.min().min() >= 0.0:\n",
    "                return \"C\"\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "        return \"A\"\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Cannot detect CSV format. Expected columns:\\n\"\n",
    "        \"  Format A: filename, xmin, ymin, xmax, ymax, class\\n\"\n",
    "        \"  Format B: filename, xmin, ymin, width, height, class\\n\"\n",
    "        \"  Format C: filename, x_center, y_center, width, height, class, normalized\"\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame, fmt: str) -> pd.DataFrame:\n",
    "    \"\"\"Rename CSV columns to standard names based on detected format.\"\"\"\n",
    "    df = df.copy()\n",
    "    cols = list(df.columns)\n",
    "    cols_lower = {c.lower().strip(): c for c in cols}\n",
    "\n",
    "    column_maps = {\n",
    "        \"A\": {\n",
    "            \"filename\": [\"filename\", \"file\", \"image\", \"image_name\", \"img\"],\n",
    "            \"xmin\": [\"xmin\", \"x_min\", \"x1\", \"left\"],\n",
    "            \"ymin\": [\"ymin\", \"y_min\", \"y1\", \"top\"],\n",
    "            \"xmax\": [\"xmax\", \"x_max\", \"x2\", \"right\"],\n",
    "            \"ymax\": [\"ymax\", \"y_max\", \"y2\", \"bottom\"],\n",
    "            \"class\": [\"class\", \"label\", \"class_name\", \"category\", \"cls\"],\n",
    "        },\n",
    "        \"B\": {\n",
    "            \"filename\": [\"filename\", \"file\", \"image\", \"image_name\", \"img\"],\n",
    "            \"xmin\": [\"xmin\", \"x_min\", \"x1\", \"left\", \"x\"],\n",
    "            \"ymin\": [\"ymin\", \"y_min\", \"y1\", \"top\", \"y\"],\n",
    "            \"width\": [\"width\", \"w\", \"box_width\"],\n",
    "            \"height\": [\"height\", \"h\", \"box_height\"],\n",
    "            \"class\": [\"class\", \"label\", \"class_name\", \"category\", \"cls\"],\n",
    "        },\n",
    "        \"C\": {\n",
    "            \"filename\": [\"filename\", \"file\", \"image\", \"image_name\", \"img\"],\n",
    "            \"x_center\": [\"x_center\", \"cx\", \"center_x\", \"x\"],\n",
    "            \"y_center\": [\"y_center\", \"cy\", \"center_y\", \"y\"],\n",
    "            \"width\": [\"width\", \"w\", \"box_width\"],\n",
    "            \"height\": [\"height\", \"h\", \"box_height\"],\n",
    "            \"class\": [\"class\", \"label\", \"class_name\", \"category\", \"cls\"],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    mapping = {}\n",
    "    for target, candidates in column_maps[fmt].items():\n",
    "        for c in candidates:\n",
    "            if c in cols_lower:\n",
    "                mapping[cols_lower[c]] = target\n",
    "                break\n",
    "\n",
    "    if len(mapping) < 6:\n",
    "        if fmt == \"A\":\n",
    "            mapping = {cols[0]: \"filename\", cols[1]: \"xmin\", cols[2]: \"ymin\",\n",
    "                       cols[3]: \"xmax\", cols[4]: \"ymax\", cols[5]: \"class\"}\n",
    "        elif fmt == \"B\":\n",
    "            mapping = {cols[0]: \"filename\", cols[1]: \"xmin\", cols[2]: \"ymin\",\n",
    "                       cols[3]: \"width\", cols[4]: \"height\", cols[5]: \"class\"}\n",
    "        elif fmt == \"C\":\n",
    "            mapping = {cols[0]: \"filename\", cols[1]: \"x_center\", cols[2]: \"y_center\",\n",
    "                       cols[3]: \"width\", cols[4]: \"height\", cols[5]: \"class\"}\n",
    "\n",
    "    df = df.rename(columns=mapping)\n",
    "    df[\"filename\"] = df[\"filename\"].apply(lambda x: os.path.basename(str(x).strip()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_class_mapping(classes_series: pd.Series) -> tuple:\n",
    "    \"\"\"\n",
    "    Build class label to integer ID mapping.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (mapping_dict, class_names_list)\n",
    "    \"\"\"\n",
    "    unique_classes = classes_series.unique()\n",
    "\n",
    "    all_int = True\n",
    "    for c in unique_classes:\n",
    "        try:\n",
    "            int(c)\n",
    "        except (ValueError, TypeError):\n",
    "            all_int = False\n",
    "            break\n",
    "\n",
    "    if all_int:\n",
    "        int_classes = sorted([int(c) for c in unique_classes])\n",
    "        if int_classes == list(range(len(int_classes))):\n",
    "            mapping = {str(c): c for c in int_classes}\n",
    "            names = [str(c) for c in int_classes]\n",
    "        else:\n",
    "            mapping = {str(c): i for i, c in enumerate(int_classes)}\n",
    "            names = [str(c) for c in int_classes]\n",
    "    else:\n",
    "        sorted_classes = sorted([str(c) for c in unique_classes])\n",
    "        mapping = {c: i for i, c in enumerate(sorted_classes)}\n",
    "        names = sorted_classes\n",
    "\n",
    "    return mapping, names\n",
    "'''\n",
    "with open(os.path.join(REPO_DIR, \"src/yolov8_pipeline/parser.py\"), 'w') as f:\n",
    "    f.write(parser_py)\n",
    "\n",
    "print(\"âœ… src/yolov8_pipeline/__init__.py\")\n",
    "print(\"âœ… src/yolov8_pipeline/parser.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ src/yolov8_pipeline/converter.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "converter_py = '''\"\"\"YOLO format conversion utilities.\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from .parser import detect_csv_format, normalize_columns, build_class_mapping\n",
    "\n",
    "\n",
    "def convert_to_yolo(row: dict, fmt: str, img_w: int, img_h: int):\n",
    "    \"\"\"\n",
    "    Convert a single annotation row to YOLO format.\n",
    "\n",
    "    Returns:\n",
    "        Tuple (x_center, y_center, width, height) normalized to [0, 1],\n",
    "        or None if the box is invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if fmt == \"A\":\n",
    "            xmin, ymin = float(row[\"xmin\"]), float(row[\"ymin\"])\n",
    "            xmax, ymax = float(row[\"xmax\"]), float(row[\"ymax\"])\n",
    "        elif fmt == \"B\":\n",
    "            xmin, ymin = float(row[\"xmin\"]), float(row[\"ymin\"])\n",
    "            xmax = xmin + float(row[\"width\"])\n",
    "            ymax = ymin + float(row[\"height\"])\n",
    "        elif fmt == \"C\":\n",
    "            xc = max(0.0, min(1.0, float(row[\"x_center\"])))\n",
    "            yc = max(0.0, min(1.0, float(row[\"y_center\"])))\n",
    "            w = max(0.0, min(1.0, float(row[\"width\"])))\n",
    "            h = max(0.0, min(1.0, float(row[\"height\"])))\n",
    "            return (xc, yc, w, h) if w > 0 and h > 0 else None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        xmin = max(0, min(img_w, xmin))\n",
    "        ymin = max(0, min(img_h, ymin))\n",
    "        xmax = max(0, min(img_w, xmax))\n",
    "        ymax = max(0, min(img_h, ymax))\n",
    "\n",
    "        bw, bh = xmax - xmin, ymax - ymin\n",
    "        if bw <= 0 or bh <= 0:\n",
    "            return None\n",
    "\n",
    "        return (\n",
    "            (xmin + xmax) / 2.0 / img_w,\n",
    "            (ymin + ymax) / 2.0 / img_h,\n",
    "            bw / img_w,\n",
    "            bh / img_h,\n",
    "        )\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_yolo_dataset(\n",
    "    images_dir: str,\n",
    "    csv_path: str,\n",
    "    output_dir: str,\n",
    "    split_ratio: float = 0.8,\n",
    "    seed: int = 42,\n",
    "    allow_background: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a complete YOLO dataset from images and CSV annotations.\n",
    "\n",
    "    Returns:\n",
    "        Path to the generated data.yaml file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    fmt = detect_csv_format(df)\n",
    "    df = normalize_columns(df, fmt)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col not in (\"filename\", \"class\"):\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    df = df.dropna()\n",
    "\n",
    "    class_mapping, class_names = build_class_mapping(df[\"class\"])\n",
    "\n",
    "    image_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "    available = {}\n",
    "    for f in os.listdir(images_dir):\n",
    "        if Path(f).suffix.lower() in image_extensions:\n",
    "            available[f] = os.path.join(images_dir, f)\n",
    "\n",
    "    df = df[df[\"filename\"].isin(available)]\n",
    "\n",
    "    for split in (\"train\", \"val\"):\n",
    "        os.makedirs(os.path.join(output_dir, \"images\", split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, \"labels\", split), exist_ok=True)\n",
    "\n",
    "    all_images = sorted(df[\"filename\"].unique())\n",
    "    random.seed(seed)\n",
    "    random.shuffle(all_images)\n",
    "    split_idx = int(len(all_images) * split_ratio)\n",
    "    train_set = set(all_images[:split_idx])\n",
    "    val_set = set(all_images[split_idx:])\n",
    "\n",
    "    if allow_background:\n",
    "        bg = set(available.keys()) - set(df[\"filename\"].unique())\n",
    "        bg_list = sorted(bg)\n",
    "        random.shuffle(bg_list)\n",
    "        bg_split = int(len(bg_list) * split_ratio)\n",
    "        train_set |= set(bg_list[:bg_split])\n",
    "        val_set |= set(bg_list[bg_split:])\n",
    "\n",
    "    for img_name in tqdm(sorted(train_set | val_set), desc=\"Converting\"):\n",
    "        split = \"train\" if img_name in train_set else \"val\"\n",
    "        src = available.get(img_name)\n",
    "        if not src:\n",
    "            continue\n",
    "\n",
    "        shutil.copy2(src, os.path.join(output_dir, \"images\", split, img_name))\n",
    "        img_w, img_h = Image.open(src).size\n",
    "\n",
    "        img_df = df[df[\"filename\"] == img_name]\n",
    "        lines = []\n",
    "        for _, row in img_df.iterrows():\n",
    "            result = convert_to_yolo(row, fmt, img_w, img_h)\n",
    "            if result:\n",
    "                cls_id = class_mapping[str(row[\"class\"])]\n",
    "                xc, yc, w, h = result\n",
    "                lines.append(f\"{cls_id} {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\")\n",
    "\n",
    "        label_name = Path(img_name).stem + \".txt\"\n",
    "        with open(os.path.join(output_dir, \"labels\", split, label_name), \"w\") as f:\n",
    "            f.write(\"\\n\".join(lines))\n",
    "\n",
    "    data_yaml = {\n",
    "        \"path\": os.path.abspath(output_dir),\n",
    "        \"train\": \"images/train\",\n",
    "        \"val\": \"images/val\",\n",
    "        \"nc\": len(class_names),\n",
    "        \"names\": class_names,\n",
    "    }\n",
    "    yaml_path = os.path.join(output_dir, \"data.yaml\")\n",
    "    with open(yaml_path, \"w\") as f:\n",
    "        yaml.dump(data_yaml, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "    return yaml_path\n",
    "'''\n",
    "with open(os.path.join(REPO_DIR, \"src/yolov8_pipeline/converter.py\"), 'w') as f:\n",
    "    f.write(converter_py)\n",
    "print(\"âœ… src/yolov8_pipeline/converter.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ src/yolov8_pipeline/visualizer.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "visualizer_py = '''\"\"\"Visualization utilities for YOLO datasets.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def visualize_yolo_labels(\n",
    "    image_path: str,\n",
    "    label_path: str,\n",
    "    class_names: list,\n",
    "    ax=None,\n",
    "    show: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Draw YOLO bounding boxes on an image.\n",
    "\n",
    "    Args:\n",
    "        image_path: Path to the image file.\n",
    "        label_path: Path to the YOLO label file.\n",
    "        class_names: List of class names ordered by ID.\n",
    "        ax: Matplotlib axes (created if None).\n",
    "        show: Whether to call plt.show().\n",
    "\n",
    "    Returns:\n",
    "        Matplotlib axes with the visualization.\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    w, h = img.size\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(os.path.basename(image_path), fontsize=9)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, max(len(class_names), 1)))\n",
    "\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                cls_id = int(parts[0])\n",
    "                xc, yc, bw, bh = [float(x) for x in parts[1:]]\n",
    "                x1 = (xc - bw / 2) * w\n",
    "                y1 = (yc - bh / 2) * h\n",
    "                rect_w = bw * w\n",
    "                rect_h = bh * h\n",
    "                color = colors[cls_id % len(colors)]\n",
    "                rect = patches.Rectangle(\n",
    "                    (x1, y1), rect_w, rect_h,\n",
    "                    linewidth=2, edgecolor=color, facecolor=\"none\",\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                label = class_names[cls_id] if cls_id < len(class_names) else str(cls_id)\n",
    "                ax.text(\n",
    "                    x1, y1 - 4, label, fontsize=8, color=\"white\",\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=color, alpha=0.8),\n",
    "                )\n",
    "\n",
    "    if show:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return ax\n",
    "'''\n",
    "with open(os.path.join(REPO_DIR, \"src/yolov8_pipeline/visualizer.py\"), 'w') as f:\n",
    "    f.write(visualizer_py)\n",
    "print(\"âœ… src/yolov8_pipeline/visualizer.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ scripts/convert_csv_to_yolo.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "convert_script = '''#!/usr/bin/env python3\n",
    "\"\"\"Convert CSV annotations to YOLO format.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path(__file__).resolve().parent.parent / \"src\"))\n",
    "\n",
    "from yolov8_pipeline.converter import create_yolo_dataset\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Convert CSV annotations to YOLO format\")\n",
    "    parser.add_argument(\"--config\", type=str, default=\"configs/default.yaml\",\n",
    "                        help=\"Path to config YAML file\")\n",
    "    parser.add_argument(\"--images-dir\", type=str, help=\"Override images directory\")\n",
    "    parser.add_argument(\"--csv-path\", type=str, help=\"Override CSV path\")\n",
    "    parser.add_argument(\"--output-dir\", type=str, help=\"Override output directory\")\n",
    "    parser.add_argument(\"--split-ratio\", type=float, help=\"Train/val split ratio\")\n",
    "    parser.add_argument(\"--seed\", type=int, help=\"Random seed\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load config\n",
    "    with open(args.config) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    images_dir = args.images_dir or config[\"paths\"][\"images_dir\"]\n",
    "    csv_path = args.csv_path or config[\"paths\"][\"csv_path\"]\n",
    "    output_dir = args.output_dir or config[\"paths\"].get(\"dataset_dir\", \"dataset\")\n",
    "    split_ratio = args.split_ratio or config[\"data\"][\"split_ratio\"]\n",
    "    seed = args.seed or config[\"data\"].get(\"seed\", 42)\n",
    "    allow_bg = config[\"data\"].get(\"allow_background\", True)\n",
    "\n",
    "    print(f\"Converting CSV annotations to YOLO format...\")\n",
    "    print(f\"  Images: {images_dir}\")\n",
    "    print(f\"  CSV:    {csv_path}\")\n",
    "    print(f\"  Output: {output_dir}\")\n",
    "\n",
    "    yaml_path = create_yolo_dataset(\n",
    "        images_dir=images_dir,\n",
    "        csv_path=csv_path,\n",
    "        output_dir=output_dir,\n",
    "        split_ratio=split_ratio,\n",
    "        seed=seed,\n",
    "        allow_background=allow_bg,\n",
    "    )\n",
    "    print(f\"\\nDone! data.yaml: {yaml_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "with open(os.path.join(REPO_DIR, \"scripts/convert_csv_to_yolo.py\"), 'w') as f:\n",
    "    f.write(convert_script)\n",
    "print(\"âœ… scripts/convert_csv_to_yolo.py\")\n",
    "\n",
    "# â”€â”€ scripts/train.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_script = '''#!/usr/bin/env python3\n",
    "\"\"\"Train YOLOv8 model.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "\n",
    "sys.path.insert(0, str(Path(__file__).resolve().parent.parent / \"src\"))\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Train YOLOv8 model\")\n",
    "    parser.add_argument(\"--config\", type=str, default=\"configs/default.yaml\",\n",
    "                        help=\"Path to config YAML file\")\n",
    "    parser.add_argument(\"--data\", type=str, help=\"Path to data.yaml\")\n",
    "    parser.add_argument(\"--model\", type=str, help=\"Model size (e.g., yolov8n.pt)\")\n",
    "    parser.add_argument(\"--imgsz\", type=int, help=\"Image size\")\n",
    "    parser.add_argument(\"--epochs\", type=int, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--batch\", type=int, help=\"Batch size\")\n",
    "    parser.add_argument(\"--device\", type=str, help=\"Device (cuda/cpu)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.config) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    tc = config[\"training\"]\n",
    "    data_path = args.data or os.path.join(\n",
    "        config[\"paths\"].get(\"dataset_dir\", \"dataset\"), \"data.yaml\"\n",
    "    )\n",
    "    model_name = args.model or tc[\"model\"]\n",
    "    imgsz = args.imgsz or tc[\"imgsz\"]\n",
    "    epochs = args.epochs or tc[\"epochs\"]\n",
    "    batch = args.batch or tc[\"batch\"]\n",
    "    device = args.device or tc.get(\"device\", \"\")\n",
    "    seed = tc.get(\"seed\", 42)\n",
    "\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "    print(f\"Training YOLOv8...\")\n",
    "    print(f\"  Model:  {model_name}\")\n",
    "    print(f\"  Data:   {data_path}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  ImgSz:  {imgsz}\")\n",
    "    print(f\"  Batch:  {batch}\")\n",
    "\n",
    "    model = YOLO(model_name)\n",
    "    results = model.train(\n",
    "        data=data_path,\n",
    "        imgsz=imgsz,\n",
    "        epochs=epochs,\n",
    "        batch=batch,\n",
    "        device=device if device else None,\n",
    "        seed=seed,\n",
    "        project=\"runs\",\n",
    "        name=\"train\",\n",
    "        exist_ok=True,\n",
    "    )\n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(f\"Best weights: runs/train/weights/best.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "with open(os.path.join(REPO_DIR, \"scripts/train.py\"), 'w') as f:\n",
    "    f.write(train_script)\n",
    "print(\"âœ… scripts/train.py\")\n",
    "\n",
    "# â”€â”€ scripts/predict.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "predict_script = '''#!/usr/bin/env python3\n",
    "\"\"\"Run YOLOv8 inference on images.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Run YOLOv8 inference\")\n",
    "    parser.add_argument(\"--weights\", type=str, required=True,\n",
    "                        help=\"Path to model weights (.pt)\")\n",
    "    parser.add_argument(\"--source\", type=str, required=True,\n",
    "                        help=\"Path to image, directory, or video\")\n",
    "    parser.add_argument(\"--conf\", type=float, default=0.25,\n",
    "                        help=\"Confidence threshold\")\n",
    "    parser.add_argument(\"--imgsz\", type=int, default=640,\n",
    "                        help=\"Inference image size\")\n",
    "    parser.add_argument(\"--save\", action=\"store_true\", default=True,\n",
    "                        help=\"Save results\")\n",
    "    parser.add_argument(\"--save-dir\", type=str, default=\"runs/predict\",\n",
    "                        help=\"Directory to save results\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "    print(f\"Running inference...\")\n",
    "    print(f\"  Weights: {args.weights}\")\n",
    "    print(f\"  Source:  {args.source}\")\n",
    "    print(f\"  Conf:    {args.conf}\")\n",
    "\n",
    "    model = YOLO(args.weights)\n",
    "    results = model.predict(\n",
    "        source=args.source,\n",
    "        conf=args.conf,\n",
    "        imgsz=args.imgsz,\n",
    "        save=args.save,\n",
    "        project=args.save_dir,\n",
    "        name=\"results\",\n",
    "        exist_ok=True,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nInference complete!\")\n",
    "    print(f\"  Processed {len(results)} images\")\n",
    "    if args.save:\n",
    "        print(f\"  Results saved to: {args.save_dir}/results/\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "with open(os.path.join(REPO_DIR, \"scripts/predict.py\"), 'w') as f:\n",
    "    f.write(predict_script)\n",
    "print(\"âœ… scripts/predict.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ configs/default.yaml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "default_config = f\"\"\"# YOLOv8 Pipeline Configuration\n",
    "# ================================\n",
    "\n",
    "paths:\n",
    "  images_dir: data/images\n",
    "  csv_path: data/annotations.csv\n",
    "  dataset_dir: dataset\n",
    "  output_dir: artifacts\n",
    "\n",
    "training:\n",
    "  model: {MODEL_SIZE}\n",
    "  imgsz: {IMGSZ}\n",
    "  epochs: {EPOCHS}\n",
    "  batch: {BATCH}\n",
    "  seed: {SEED}\n",
    "  device: \"\"  # auto\n",
    "\n",
    "data:\n",
    "  split_ratio: {SPLIT_RATIO}\n",
    "  allow_background: {str(ALLOW_BACKGROUND_IMAGES).lower()}\n",
    "\n",
    "export:\n",
    "  formats:\n",
    "    - onnx\n",
    "    - torchscript\n",
    "\"\"\"\n",
    "with open(os.path.join(REPO_DIR, \"configs/default.yaml\"), 'w') as f:\n",
    "    f.write(default_config)\n",
    "print(\"âœ… configs/default.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ docs/dataset_format.md â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "dataset_format_doc = \"\"\"# Dataset Format Guide\n",
    "\n",
    "## Overview\n",
    "\n",
    "This pipeline accepts image datasets with bounding box annotations provided in CSV format.\n",
    "The CSV format is **automatically detected** based on column names.\n",
    "\n",
    "---\n",
    "\n",
    "## Image Requirements\n",
    "\n",
    "- **Supported formats**: JPG, JPEG, PNG, BMP, TIFF, WebP\n",
    "- **Location**: Place all images in a single directory\n",
    "- **Naming**: Filenames in the CSV must match the image filenames (basename only)\n",
    "\n",
    "---\n",
    "\n",
    "## CSV Annotation Formats\n",
    "\n",
    "### Format A: Pixel Coordinates (Corners)\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `filename` | string | Image filename (e.g., `img001.jpg`) |\n",
    "| `xmin` | float | Left edge of bounding box (pixels) |\n",
    "| `ymin` | float | Top edge of bounding box (pixels) |\n",
    "| `xmax` | float | Right edge of bounding box (pixels) |\n",
    "| `ymax` | float | Bottom edge of bounding box (pixels) |\n",
    "| `class` | string/int | Class label or ID |\n",
    "\n",
    "**Example:**\n",
    "```csv\n",
    "filename,xmin,ymin,xmax,ymax,class\n",
    "img001.jpg,100,50,300,200,cat\n",
    "img001.jpg,400,100,550,350,dog\n",
    "img002.jpg,50,50,200,180,cat\n",
    "```\n",
    "\n",
    "### Format B: Pixel Coordinates (Top-Left + Size)\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `filename` | string | Image filename |\n",
    "| `xmin` | float | Left edge (pixels) |\n",
    "| `ymin` | float | Top edge (pixels) |\n",
    "| `width` | float | Box width (pixels) |\n",
    "| `height` | float | Box height (pixels) |\n",
    "| `class` | string/int | Class label or ID |\n",
    "\n",
    "**Example:**\n",
    "```csv\n",
    "filename,xmin,ymin,width,height,class\n",
    "img001.jpg,100,50,200,150,cat\n",
    "img001.jpg,400,100,150,250,dog\n",
    "```\n",
    "\n",
    "### Format C: Normalized YOLO Format\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `filename` | string | Image filename |\n",
    "| `x_center` | float | Center X (normalized 0-1) |\n",
    "| `y_center` | float | Center Y (normalized 0-1) |\n",
    "| `width` | float | Box width (normalized 0-1) |\n",
    "| `height` | float | Box height (normalized 0-1) |\n",
    "| `class` | string/int | Class label or ID |\n",
    "| `normalized` | any | Flag column (presence triggers Format C detection) |\n",
    "\n",
    "**Example:**\n",
    "```csv\n",
    "filename,x_center,y_center,width,height,class,normalized\n",
    "img001.jpg,0.3125,0.2604,0.3125,0.3125,cat,true\n",
    "img001.jpg,0.7422,0.4688,0.2344,0.5208,dog,true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Class Labels\n",
    "\n",
    "- **String labels**: Automatically sorted alphabetically and mapped to integer IDs\n",
    "- **Integer labels**: Validated for contiguous range starting from 0; remapped if necessary\n",
    "\n",
    "---\n",
    "\n",
    "## Multiple Objects Per Image\n",
    "\n",
    "Multiple rows with the same filename represent multiple objects in that image.\n",
    "Each row becomes one bounding box in the YOLO label file.\n",
    "\n",
    "---\n",
    "\n",
    "## Column Name Flexibility\n",
    "\n",
    "The parser recognizes common column name variants:\n",
    "\n",
    "| Standard | Also Accepted |\n",
    "|----------|---------------|\n",
    "| `filename` | `file`, `image`, `image_name`, `img` |\n",
    "| `xmin` | `x_min`, `x1`, `left` |\n",
    "| `ymin` | `y_min`, `y1`, `top` |\n",
    "| `xmax` | `x_max`, `x2`, `right` |\n",
    "| `ymax` | `y_max`, `y2`, `bottom` |\n",
    "| `width` | `w`, `box_width` |\n",
    "| `height` | `h`, `box_height` |\n",
    "| `class` | `label`, `class_name`, `category`, `cls` |\n",
    "\"\"\"\n",
    "with open(os.path.join(REPO_DIR, \"docs/dataset_format.md\"), 'w') as f:\n",
    "    f.write(dataset_format_doc)\n",
    "print(\"âœ… docs/dataset_format.md\")\n",
    "\n",
    "# â”€â”€ docs/usage_guide.md â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "usage_guide_doc = \"\"\"# Usage Guide\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Prerequisites](#prerequisites)\n",
    "2. [Installation](#installation)\n",
    "3. [Preparing Your Data](#preparing-your-data)\n",
    "4. [Converting Annotations](#converting-annotations)\n",
    "5. [Training](#training)\n",
    "6. [Evaluation](#evaluation)\n",
    "7. [Inference](#inference)\n",
    "8. [Exporting Models](#exporting-models)\n",
    "9. [Using as a Python Package](#using-as-a-python-package)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.8 or higher\n",
    "- CUDA-capable GPU (recommended for training)\n",
    "- pip package manager\n",
    "\n",
    "---\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "# Clone the repository\n",
    "git clone https://github.com/m0d9/yolov8-pipeline.git\n",
    "cd yolov8-pipeline\n",
    "\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Or install as a package\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Preparing Your Data\n",
    "\n",
    "1. Create a `data/` directory in the project root\n",
    "2. Place your images in `data/images/`\n",
    "3. Create `data/annotations.csv` with your bounding box annotations\n",
    "\n",
    "See `docs/dataset_format.md` for supported CSV formats.\n",
    "\n",
    "---\n",
    "\n",
    "## Converting Annotations\n",
    "\n",
    "```bash\n",
    "# Using the default config\n",
    "python scripts/convert_csv_to_yolo.py --config configs/default.yaml\n",
    "\n",
    "# With custom paths\n",
    "python scripts/convert_csv_to_yolo.py \\\\\n",
    "    --images-dir /path/to/images \\\\\n",
    "    --csv-path /path/to/annotations.csv \\\\\n",
    "    --output-dir /path/to/dataset \\\\\n",
    "    --split-ratio 0.8\n",
    "```\n",
    "\n",
    "This creates the YOLO dataset structure with train/val splits and generates `data.yaml`.\n",
    "\n",
    "---\n",
    "\n",
    "## Training\n",
    "\n",
    "```bash\n",
    "# Using config file\n",
    "python scripts/train.py --config configs/default.yaml\n",
    "\n",
    "# With overrides\n",
    "python scripts/train.py \\\\\n",
    "    --data dataset/data.yaml \\\\\n",
    "    --model yolov8s.pt \\\\\n",
    "    --epochs 100 \\\\\n",
    "    --batch 32 \\\\\n",
    "    --imgsz 640\n",
    "```\n",
    "\n",
    "Training results are saved to `runs/train/`.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "After training, evaluate on the validation set:\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"runs/train/weights/best.pt\")\n",
    "metrics = model.val(data=\"dataset/data.yaml\")\n",
    "\n",
    "print(f\"mAP50: {metrics.box.map50}\")\n",
    "print(f\"mAP50-95: {metrics.box.map}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Inference\n",
    "\n",
    "```bash\n",
    "# On a single image\n",
    "python scripts/predict.py --weights artifacts/best.pt --source image.jpg\n",
    "\n",
    "# On a directory\n",
    "python scripts/predict.py --weights artifacts/best.pt --source images/\n",
    "\n",
    "# With custom confidence threshold\n",
    "python scripts/predict.py --weights artifacts/best.pt --source images/ --conf 0.5\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Exporting Models\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"artifacts/best.pt\")\n",
    "\n",
    "# Export to various formats\n",
    "model.export(format=\"onnx\")         # ONNX\n",
    "model.export(format=\"torchscript\")  # TorchScript\n",
    "model.export(format=\"tflite\")       # TensorFlow Lite\n",
    "model.export(format=\"engine\")       # TensorRT (requires TensorRT)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Using as a Python Package\n",
    "\n",
    "```python\n",
    "from yolov8_pipeline import (\n",
    "    detect_csv_format,\n",
    "    normalize_columns,\n",
    "    build_class_mapping,\n",
    "    convert_to_yolo,\n",
    "    create_yolo_dataset,\n",
    "    visualize_yolo_labels,\n",
    ")\n",
    "\n",
    "# Create a complete YOLO dataset from CSV\n",
    "yaml_path = create_yolo_dataset(\n",
    "    images_dir=\"data/images\",\n",
    "    csv_path=\"data/annotations.csv\",\n",
    "    output_dir=\"dataset\",\n",
    "    split_ratio=0.8,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Or use individual functions\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/annotations.csv\")\n",
    "fmt = detect_csv_format(df)\n",
    "df = normalize_columns(df, fmt)\n",
    "mapping, names = build_class_mapping(df[\"class\"])\n",
    "```\n",
    "\"\"\"\n",
    "with open(os.path.join(REPO_DIR, \"docs/usage_guide.md\"), 'w') as f:\n",
    "    f.write(usage_guide_doc)\n",
    "print(\"âœ… docs/usage_guide.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Hugging Face Model Card â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model_card = f\"\"\"---\n",
    "license: {LICENSE_TYPE.lower()}\n",
    "tags:\n",
    "  - object-detection\n",
    "  - yolov8\n",
    "  - ultralytics\n",
    "  - computer-vision\n",
    "  - pytorch\n",
    "library_name: ultralytics\n",
    "pipeline_tag: object-detection\n",
    "---\n",
    "\n",
    "# YOLOv8 Object Detection Model\n",
    "\n",
    "## Model Description\n",
    "\n",
    "This is a **YOLOv8** object detection model trained using the [Ultralytics](https://github.com/ultralytics/ultralytics) framework. The model was trained on a custom dataset with CSV-formatted annotations, converted to YOLO format using an automated pipeline.\n",
    "\n",
    "### Model Details\n",
    "\n",
    "- **Architecture**: YOLOv8 ({MODEL_SIZE.replace('.pt', '')})\n",
    "- **Input Size**: {IMGSZ}x{IMGSZ}\n",
    "- **Number of Classes**: {{num_classes}}\n",
    "- **Classes**: {{', '.join(class_names)}}\n",
    "- **Framework**: Ultralytics / PyTorch\n",
    "\n",
    "## Training Details\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Model | {MODEL_SIZE} |\n",
    "| Image Size | {IMGSZ} |\n",
    "| Epochs | {EPOCHS} |\n",
    "| Batch Size | {BATCH} |\n",
    "| Seed | {SEED} |\n",
    "| Train/Val Split | {SPLIT_RATIO}/{1-SPLIT_RATIO:.1f} |\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "This model is intended for object detection tasks on images similar to the training data. It can detect and localize objects with bounding boxes and class labels.\n",
    "\n",
    "### Primary Use Cases\n",
    "\n",
    "- Real-time object detection in images and video\n",
    "- Batch processing of image datasets\n",
    "- Integration into computer vision pipelines\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Performance may degrade on images significantly different from the training distribution\n",
    "- The model is optimized for the specific classes it was trained on\n",
    "- Real-time performance depends on hardware (GPU recommended)\n",
    "\n",
    "## How to Use\n",
    "\n",
    "### Python API\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the model\n",
    "model = YOLO(\"best.pt\")\n",
    "\n",
    "# Run inference\n",
    "results = model.predict(\"image.jpg\", conf=0.25)\n",
    "\n",
    "# Process results\n",
    "for r in results:\n",
    "    boxes = r.boxes\n",
    "    for box in boxes:\n",
    "        cls = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        xyxy = box.xyxy[0].tolist()\n",
    "        print(f\"Class: {{cls}}, Confidence: {{conf:.2f}}, Box: {{xyxy}}\")\n",
    "```\n",
    "\n",
    "### Command Line\n",
    "\n",
    "```bash\n",
    "yolo predict model=best.pt source=image.jpg conf=0.25\n",
    "```\n",
    "\n",
    "## Export Formats\n",
    "\n",
    "The model can be exported to:\n",
    "\n",
    "| Format | File | Notes |\n",
    "|--------|------|-------|\n",
    "| PyTorch | `best.pt` | Default format |\n",
    "| ONNX | `best.onnx` | Cross-platform inference |\n",
    "| TorchScript | `best.torchscript` | C++ deployment |\n",
    "| TensorFlow Lite | `best.tflite` | Mobile deployment |\n",
    "| TensorRT | `best.engine` | NVIDIA GPU optimization |\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this model, please cite:\n",
    "\n",
    "```bibtex\n",
    "@software{{ultralytics_yolov8,\n",
    "  author = {{Jocher, Glenn and Chaurasia, Ayush and Qiu, Jing}},\n",
    "  title = {{Ultralytics YOLOv8}},\n",
    "  year = {{2023}},\n",
    "  url = {{https://github.com/ultralytics/ultralytics}},\n",
    "  license = {{AGPL-3.0}}\n",
    "}}\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "This model is released under the {LICENSE_TYPE} license. See the LICENSE file for details.\n",
    "\n",
    "> **Note**: The Ultralytics YOLOv8 framework is licensed under AGPL-3.0.\n",
    "> If you use the pretrained weights, please review the Ultralytics license terms.\n",
    "\"\"\"\n",
    "\n",
    "# Save as model-card.md in repo root\n",
    "with open(os.path.join(REPO_DIR, \"model-card.md\"), 'w') as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "# Also save as README.md in huggingface/ folder\n",
    "with open(os.path.join(REPO_DIR, \"huggingface/README.md\"), 'w') as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\"âœ… model-card.md\")\n",
    "print(\"âœ… huggingface/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Copy artifacts into repository â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "artifacts_repo = os.path.join(REPO_DIR, \"artifacts\")\n",
    "\n",
    "for fname in os.listdir(OUTPUT_DIR):\n",
    "    src = os.path.join(OUTPUT_DIR, fname)\n",
    "    dst = os.path.join(artifacts_repo, fname)\n",
    "    shutil.copy2(src, dst)\n",
    "    print(f\"   ğŸ“¦ {fname}\")\n",
    "\n",
    "# Copy data.yaml\n",
    "shutil.copy2(data_yaml_path, os.path.join(artifacts_repo, \"data.yaml\"))\n",
    "\n",
    "print(f\"\\nâœ… Artifacts copied to {artifacts_repo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Display final repository structure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def print_tree(path, prefix=\"\", max_depth=4, depth=0):\n",
    "    if depth >= max_depth:\n",
    "        return\n",
    "    entries = sorted(os.listdir(path))\n",
    "    dirs = [e for e in entries if os.path.isdir(os.path.join(path, e))]\n",
    "    files = [e for e in entries if os.path.isfile(os.path.join(path, e))]\n",
    "\n",
    "    for f in files:\n",
    "        size = os.path.getsize(os.path.join(path, f))\n",
    "        size_str = f\"{size/1024:.1f}KB\" if size > 1024 else f\"{size}B\"\n",
    "        print(f\"{prefix}ğŸ“„ {f} ({size_str})\")\n",
    "\n",
    "    for i, d in enumerate(dirs):\n",
    "        is_last = (i == len(dirs) - 1)\n",
    "        print(f\"{prefix}ğŸ“‚ {d}/\")\n",
    "        new_prefix = prefix + (\"    \" if is_last else \"â”‚   \")\n",
    "        print_tree(os.path.join(path, d), new_prefix, max_depth, depth + 1)\n",
    "\n",
    "print(f\"\\nğŸ“ Repository Structure: {REPO_DIR}\")\n",
    "print(\"=\" * 60)\n",
    "print_tree(REPO_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 Â· Package & Download\n",
    "\n",
    "Zip the repository and artifacts for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Zip the repository â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import zipfile\n",
    "\n",
    "def zip_directory(source_dir, output_path):\n",
    "    \"\"\"Create a zip file from a directory.\"\"\"\n",
    "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(source_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, os.path.dirname(source_dir))\n",
    "                zipf.write(file_path, arcname)\n",
    "    size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    print(f\"   ğŸ“¦ {output_path} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(\"Packaging repository and artifacts...\\n\")\n",
    "\n",
    "# Zip repository\n",
    "repo_zip = \"/content/yolov8_pipeline_repo.zip\"\n",
    "zip_directory(REPO_DIR, repo_zip)\n",
    "\n",
    "# Zip artifacts only\n",
    "artifacts_zip = \"/content/yolov8_trained_artifacts.zip\"\n",
    "zip_directory(OUTPUT_DIR, artifacts_zip)\n",
    "\n",
    "print(f\"\\nâœ… Ready for download!\")\n",
    "print(f\"   ğŸ“¥ Repository:  {repo_zip}\")\n",
    "print(f\"   ğŸ“¥ Artifacts:   {artifacts_zip}\")\n",
    "\n",
    "# Download in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"\\nâ¬‡ï¸  Downloading repository zip...\")\n",
    "    files.download(repo_zip)\n",
    "    print(\"â¬‡ï¸  Downloading artifacts zip...\")\n",
    "    files.download(artifacts_zip)\n",
    "except ImportError:\n",
    "    print(\"\\nğŸ’¡ Not running in Colab. Files are saved at the paths above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Â· Summary\n",
    "\n",
    "### What was created:\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **YOLO Dataset** | Images + labels in train/val splits with `data.yaml` |\n",
    "| **Trained Model** | `best.pt` weights with logged metrics |\n",
    "| **Exports** | ONNX and TorchScript formats |\n",
    "| **Repository** | Complete project structure for GitHub & Hugging Face |\n",
    "| **Documentation** | README, dataset format guide, usage guide, model card |\n",
    "| **Scripts** | Standalone conversion, training, and prediction scripts |\n",
    "| **Package** | Installable Python package with reusable functions |\n",
    "| **Configs** | YAML configuration for reproducible experiments |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **GitHub**: Unzip the repository, `git init`, and push to GitHub\n",
    "2. **Hugging Face**: Upload the `huggingface/` folder contents to a new model repository\n",
    "3. **Fine-tune**: Adjust hyperparameters in `configs/default.yaml` and retrain\n",
    "4. **Deploy**: Use the ONNX export for production inference\n",
    "\n",
    "---\n",
    "> ğŸ‰ **Pipeline complete!** Your professional YOLOv8 repository is ready."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
